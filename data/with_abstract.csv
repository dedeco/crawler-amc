type,id,author,editor,advisor,note,title,pages,article_no,num_pages,keywords,doi,journal,issue_date,volume,issue_no,description,month,year,issn,booktitle,acronym,edition,isbn,conf_loc,publisher,publisher_loc,abstract
article,2558048,Jaedong  Lee and Sora  Hong and Jee-Hyong  Lee,,,,An Efficient Prediction for Heavy Rain from Big Weather Data Using Genetic Algorithm,25:1--25:7,25,7,"accuracy, big data mining, computational cost, feature selection, genetic algorithm, heavy rain prediction, support vector machine",10.1145/2557977.2558048,,,,,,,2014,,Proceedings of the 8th International Conference on Ubiquitous Information Management and Communication,ICUIMC '14,,978-1-4503-2644-5,"Siem Reap, Cambodia",ACM,"New York, NY, USA","In this paper, an approach is proposed which builds an efficient and effective model for heavy rain forecasting in 6 hours based on the past weather data. Since the weather data has a huge amount of information, in order to build efficient and effective heavy rain prediction models, we need to find proper weather attributes, isobaric surfaces and suitable range for experiments. First, we have the candidate weather attributes with expert knowledge and evaluate those attributes using machine learning approach, Support Vector Machine (SVM). After the evaluation, we find out the best resulted 3 pairs of an attribute and an isobaric surface, and combine those to have better performance in prediction. The combination of high performed weather attributes showed better performance than the combined attributes which was recommended by the experts. We next figure out how the range of area affect in prediction. After the experiments, dimensions of the best resulted data were 4,800, which will be used as the inputs to prediction models. Even though we have dramatically reduced the number of dimension compared to the original weather data, it still is not proper for heavy rain forecasting in 6 hours. The running time of the model to produce an output with 4,800 dimensions of input takes about 2 minutes. However, 2 minute is not short enough since every local place may needs to predict heavy rain with their own local heavy rain cases for more accurate weather forecasting. If there are 30 local places, it would take an hour to produce outputs for all local places. An hour is not feasible to predict heavy rain in 6 hours. Therefore, in order to build more efficient models, we apply Genetic Algorithm (GA) to find a much smaller set of inputs without degrading performance. After running GA, 4,800 inputs are reduced 757 inputs and the running time of the model with 757 inputs is 1/8 of the model with 4,800 inputs. Finally, we compare the performance between the proposed GA and the information gain (IG) based feature selection method and prove our proposed GA selected more efficient features to predict heavy rain cases."
article,3018741,Rohit  Babbar and Bernhard  Sch&#246;lkopf,,,,DiSMEC: Distributed Sparse Machines for Extreme Multi-label Classification,721--729,,9,"extreme classification, large-scale classification, multi-label learning",10.1145/3018661.3018741,,,,,,,2017,,Proceedings of the Tenth ACM International Conference on Web Search and Data Mining,WSDM '17,,978-1-4503-4675-7,"Cambridge, United Kingdom",ACM,"New York, NY, USA","Extreme multi-label classification refers to supervised multi-label learning involving hundreds of thousands or even millions of labels. Datasets in extreme classification exhibit fit to power-law distribution, i.e. a large fraction of labels have very few positive instances in the data distribution. Most state-of-the-art approaches for extreme multi-label classification attempt to capture correlation among labels by embedding the label matrix to a low-dimensional linear sub-space. However, in the presence of power-law distributed extremely large and diverse label spaces, structural assumptions such as low rank can be easily violated. In this work, we present DiSMEC, which is a large-scale distributed framework for learning one-versus-rest linear classifiers coupled with explicit capacity control to control model size. Unlike most state-of-the-art methods, DiSMEC does not make any low rank assumptions on the label matrix. Using double layer of parallelization, DiSMEC can learn classifiers for datasets consisting hundreds of thousands labels within few hours. The explicit capacity control mechanism filters out spurious parameters which keep the model compact in size, without losing prediction accuracy. We conduct extensive empirical evaluation on publicly available real-world datasets consisting upto 670,000 labels. We compare DiSMEC with recent state-of-the-art approaches, including - SLEEC which is a leading approach for learning sparse local embeddings, and FastXML which is a tree-based approach optimizing ranking based loss function. On some of the datasets, DiSMEC can significantly boost prediction accuracies - 10% better compared to SLECC and 15% better compared to FastXML, in absolute terms."
article,3097987,Yukihiro  Tagami,,,,AnnexML: Approximate Nearest Neighbor Search for Extreme Multi-label Classification,455--464,,10,"approximate nearest neighbor search, extreme multi-label classification, k-nearest neighbor graph, learning-to-rank",10.1145/3097983.3097987,,,,,,,2017,,Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,KDD '17,,978-1-4503-4887-4,"Halifax, NS, Canada",ACM,"New York, NY, USA","Extreme multi-label classification methods have been widely used in Web-scale classification tasks such as Web page tagging and product recommendation. In this paper, we present a novel graph embedding method called ""AnnexML"". At the training step, AnnexML constructs a k-nearest neighbor graph of label vectors and attempts to reproduce the graph structure in the embedding space. The prediction is efficiently performed by using an approximate nearest neighbor search method that efficiently explores the learned k-nearest neighbor graph in the embedding space. We conducted evaluations on several large-scale real-world data sets and compared our method with recent state-of-the-art methods. Experimental results show that our AnnexML can significantly improve prediction accuracy, especially on data sets that have larger a label space. In addition, AnnexML improves the trade-off between prediction time and accuracy. At the same level of accuracy, the prediction time of AnnexML was up to 58 times faster than that of SLEEC, which is a state-of-the-art embedding-based method."
article,2623651,Yashoteja  Prabhu and Manik  Varma,,,,"FastXML: A Fast, Accurate and Stable Tree-classifier for Extreme Multi-label Learning",263--272,,10,"extreme classification, multi-label learning, ranking",10.1145/2623330.2623651,,,,,,,2014,,Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,KDD '14,,978-1-4503-2956-9,"New York, New York, USA",ACM,"New York, NY, USA","The objective in extreme multi-label classification is to learn a classifier that can automatically tag a data point with the most relevant subset of labels from a large label set. Extreme multi-label classification is an important research problem since not only does it enable the tackling of applications with many labels but it also allows the reformulation of ranking problems with certain advantages over existing formulations. Our objective, in this paper, is to develop an extreme multi-label classifier that is faster to train and more accurate at prediction than the state-of-the-art Multi-label Random Forest (MLRF) algorithm [2] and the Label Partitioning for Sub-linear Ranking (LPSR) algorithm [35]. MLRF and LPSR learn a hierarchy to deal with the large number of labels but optimize task independent measures, such as the Gini index or clustering error, in order to learn the hierarchy. Our proposed FastXML algorithm achieves significantly higher accuracies by directly optimizing an nDCG based ranking loss function. We also develop an alternating minimization algorithm for efficiently optimizing the proposed formulation. Experiments reveal that FastXML can be trained on problems with more than a million labels on a standard desktop in eight hours using a single core and in an hour using multiple cores."
article,2939756,Himanshu  Jain and Yashoteja  Prabhu and Manik  Varma,,,,"Extreme Multi-label Loss Functions for Recommendation, Tagging, Ranking &#38; Other Missing Label Applications",935--944,,10,"extreme classification, multi-label learning, ranking, recommendation, tagging",10.1145/2939672.2939756,,,,,,,2016,,Proceedings of the 22Nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,KDD '16,,978-1-4503-4232-2,"San Francisco, California, USA",ACM,"New York, NY, USA","The choice of the loss function is critical in extreme multi-label learning where the objective is to annotate each data point with the most relevant subset of labels from an extremely large label set. Unfortunately, existing loss functions, such as the Hamming loss, are unsuitable for learning, model selection, hyperparameter tuning and performance evaluation. This paper addresses the issue by developing propensity scored losses which: (a) prioritize predicting the few relevant labels over the large number of irrelevant ones; (b) do not erroneously treat missing labels as irrelevant but instead provide unbiased estimates of the true loss function even when ground truth labels go missing under arbitrary probabilistic label noise models; and (c) promote the accurate prediction of infrequently occurring, hard to predict, but rewarding tail labels. Another contribution is the development of algorithms which efficiently scale to extremely large datasets with up to 9 million labels, 70 million points and 2 million dimensions and which give significant improvements over the state-of-the-art. This paper's results also apply to tagging, recommendation and ranking which are the motivating applications for extreme multi-label learning. They generalize previous attempts at deriving unbiased losses under the restrictive assumption that labels go missing uniformly at random from the ground truth. Furthermore, they provide a sound theoretical justification for popular label weighting heuristics used to recommend rare items. Finally, they demonstrate that the proposed contributions align with real world applications by achieving superior clickthrough rates on sponsored search advertising in Bing."
article,3098144,Alban  Siffer and Pierre-Alain  Fouque and Alexandre  Termier and Christine  Largouet,,,,Anomaly Detection in Streams with Extreme Value Theory,1067--1075,,9,"extreme value theory, outliers in time series, streaming",10.1145/3097983.3098144,,,,,,,2017,,Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,KDD '17,,978-1-4503-4887-4,"Halifax, NS, Canada",ACM,"New York, NY, USA","Anomaly detection in time series has attracted considerable attention due to its importance in many real-world applications including intrusion detection, energy management and finance. Most approaches for detecting outliers rely on either manually set thresholds or assumptions on the distribution of data according to Chandola, Banerjee and Kumar. Here, we propose a new approach to detect outliers in streaming univariate time series based on Extreme Value Theory that does not require to hand-set thresholds and makes no assumption on the distribution: the main parameter is only the risk, controlling the number of false positives. Our approach can be used for outlier detection, but more generally for automatically setting thresholds, making it useful in wide number of situations. We also experiment our algorithms on various real-world datasets which confirm its soundness and efficiency."
article,3098083,Ian E.H. Yen and Xiangru  Huang and Wei  Dai and Pradeep  Ravikumar and Inderjit  Dhillon and Eric  Xing,,,,PPDsparse: A Parallel Primal-Dual Sparse Method for Extreme Classification,545--553,,9,"extreme classification, primal dual method, sparse optimization",10.1145/3097983.3098083,,,,,,,2017,,Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,KDD '17,,978-1-4503-4887-4,"Halifax, NS, Canada",ACM,"New York, NY, USA","Extreme Classification comprises multi-class or multi-label prediction where there is a large number of classes, and is increasingly relevant to many real-world applications such as text and image tagging. In this setting, standard classification methods, with complexity linear in the number of classes, become intractable, while enforcing structural constraints among classes (such as low-rank or tree-structure) to reduce complexity often sacrifices accuracy for efficiency. The recent PD-Sparse method addresses this via an algorithm that is sub-linear in the number of variables, by exploiting primal-dual sparsity inherent in a specific loss function, namely the max-margin loss. In this work, we extend PD-Sparse to be efficiently parallelized in large-scale distributed settings. By introducing separable loss functions, we can scale out the training, with network communication and space efficiency comparable to those in one-versus-all approaches while maintaining an overall complexity sub-linear in the number of classes. On several large-scale benchmarks our proposed method achieves accuracy competitive to the state-of-the-art while reducing the training time from days to tens of minutes compared with existing parallel or sparse methods on a cluster of 100 cores."
article,2023581,"David John Gagne,II and Amy  McGovern and Ming  Xue",,,,Machine Learning Enhancement of Storm Scale Ensemble Precipitation Forecasts,45--46,,2,"enhancement, forecasts, machine learning, precipitation, storm scale ensemble",10.1145/2023568.2023581,,,,,,,2011,,"Proceedings of the 2011 Workshop on Knowledge Discovery, Modeling and Simulation",KDMS '11,,978-1-4503-0836-6,"San Diego, California, USA",ACM,"New York, NY, USA","Precipitation intensity forecasting is among the most challenging for meteorologists because of two main sources of complexity. First, the formation of precipitation requires the conversion of atmospheric moisture into water that falls to the ground and sometimes requires upward vertical motion. Multiple mechanisms can produce, enhance, or interfere with the precipitation processes, and those mechanisms operate on many scales. Numerical models explicitly represent grid resolvable scales but have to parameterize smaller-scale processes with physical and statistical relationships. Denser grids mean fewer parameterizations, but some processes are too small to resolve feasibly. These model resolution and parameterization limitations result in precipitation prediction errors. Second, the limited number of observations to initialize the models and error in such observations results in initial condition imperfections that grow as the model is run. Numerical model ensembles were developed to quantify the range of possible errors stemming from these sources by perturbing the initial conditions and varying the parameterizations of each model. Sorting the contributions of those error sources requires an archive of previous ensemble forecasts and post-processing to discover patterns and translate them into a calibrated probabilistic forecast. Machine learning techniques provide ways to perform both tasks with varying degrees of skill and understanding. This project compares the ability of multiple machine learning techniques to produce skilled probabilistic precipitation forecasts from a high resolution ensemble of numerical model forecasts and to discover patterns in the ensemble that reveal its strengths and weaknesses as well as where the largest influences in the forecast system lie. The application of machine learning techniques to the problem of ensemble post-processing has so far been very limited in the field of numerical weather prediction (NWP)."
article,2350184,Debasish  Das and Auroop  Ganguly and Arindam  Banerjee and Zoran  Obradovic,,,,Towards Understanding Dominant Processes in Complex Dynamical Systems: Case of Precipitation Extremes,16--24,,9,"dominant processes, graphical models, precipitation extremes, predictive models",10.1145/2350182.2350184,,,,,,,2012,,Proceedings of the Sixth International Workshop on Knowledge Discovery from Sensor Data,SensorKDD '12,,978-1-4503-1554-8,"Beijing, China",ACM,"New York, NY, USA","Complex dynamical systems like precipitation extremes under climate variability or change are typically governed by multiple processes at multiple scales. The processes themselves may be manifested at multiple scales and would need to be captured through key indicator variables, which in turn may be better projected by physical models than the variables of interest. We posit that hybrid approaches based on physically-motivated approaches and data-driven methods, which in turn are conditioned on both observations and simulations from large-scale physics-based models, may offer novel and quantifiable insights. The data-driven approaches may need to extend and adapt methods developed and tested in statistics, data mining and machine learning to the concept of dominant processes. In this paper, we performed some exploratory data analysis to characterize the effect of dominant processes on precipitation extremes, annually and seasonally, and from global and century scale to regional and decadal scale, and found some interesting insights that pointed towards need of improved understanding. We identified the gaps in understanding the regional drivers where data-driven methods can make useful improvements and eventually lead to a predictive model for precipitation extremes. Although we do not propose any specific method for solving the problem, we realize that any successful data mining solution should include all or a subset of tools like dimensionality reduction, grouped variable selection, non-linear regression and graphical models. The concepts of dominant processes proposed here would likely generalize broadly to climate extremes while the solution frameworks themselves may generalize beyond climate."
article,2447492,Kulsawasd  Jitkajornwanich and Ramez  Elmasri and John  McEnery and Chengkai  Li,,,,Extracting Storm-centric Characteristics from Raw Rainfall Data for Storm Analysis and Mining,91--99,,9,"CUAHSI, ODM, precipitation, rainfall, storm analysis",10.1145/2447481.2447492,,,,,,,2012,,Proceedings of the 1st ACM SIGSPATIAL International Workshop on Analytics for Big Geospatial Data,BigSpatial '12,,978-1-4503-1692-7,"Redondo Beach, California",ACM,"New York, NY, USA","Most rainfall data is stored in formats that are not easy to analyze and mine. In these formats, the amount of data is enormous. In this paper, we propose techniques to summarize the raw rainfall data into a model that facilitates storm analysis and mining, and reduces the data size. The result is to convert raw rainfall data into meaningful storm-centric data, which is then stored in a relational database for easy analysis and mining. The size of the storm data is less than 1% of the size of the raw data. We can determine the spatio-temporal characteristics of a storm, such as how big a storm is, how many sites are covered, and what is its overall depth (precipitation) and duration. We present formal definitions for the storm-related concepts that are needed in our data conversion. Then we describe storm identification algorithms based on these concepts. Our storm identification algorithms analyze precipitation values of adjacent sites within the period of time that covers the whole storm and combines them together to identify the overall storm characteristics."
article,1862923,Radu  Berinde and Piotr  Indyk and Graham  Cormode and Martin J. Strauss,,,,Space-optimal Heavy Hitters with Strong Error Bounds,26:1--26:28,26,28,"Frequency estimation, heavy hitters, streaming algorithms",10.1145/1862919.1862923,ACM Trans. Database Syst.,November 2010,35,4,,October,2010,0362-5915,,,,,,ACM,"New York, NY, USA","The problem of finding heavy hitters and approximating the frequencies of items is at the heart of many problems in data stream analysis. It has been observed that several proposed solutions to this problem can outperform their worst-case guarantees on real data. This leads to the question of whether some stronger bounds can be guaranteed. We answer this in the positive by showing that a class of counter-based algorithms (including the popular and very space-efficient Frequent and SpacesSaving algorithms) provides much stronger approximation guarantees than previously known. Specifically, we show that errors in the approximation of individual elements do not depend on the frequencies of the most frequent elements, but only on the frequency of the remaining tail. This shows that counter-based methods are the most space-efficient (in fact, space-optimal) algorithms having this strong error bound. This tail guarantee allows these algorithms to solve the sparse recovery problem. Here, the goal is to recover a faithful representation of the vector of frequencies, f. We prove that using space O(k), the algorithms construct an approximation f* to the frequency vector f so that the L1 error ∥∥f−∥f*∥1 is close to the best possible error minf′ ∥f′ − f∥1, where f′ ranges over all vectors with at most k non-zero entries. This improves the previously best known space bound of about O(k log n) for streams without element deletions (where n is the size of the domain from which stream elements are drawn). Other consequences of the tail guarantees are results for skewed (Zipfian) data, and guarantees for accuracy of merging multiple summarized streams."
article,1559819,Radu  Berinde and Graham  Cormode and Piotr  Indyk and Martin J. Strauss,,,,Space-optimal Heavy Hitters with Strong Error Bounds,157--166,,10,"frequency estimation, heavy hitters, streaming algorithms",10.1145/1559795.1559819,,,,,,,2009,,Proceedings of the Twenty-eighth ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems,PODS '09,,978-1-60558-553-6,"Providence, Rhode Island, USA",ACM,"New York, NY, USA","The problem of finding heavy hitters and approximating the frequencies of items is at the heart of many problems in data stream analysis. It has been observed that several proposed solutions to this problem can outperform their worst-case guarantees on real data. This leads to the question of whether some stronger bounds can be guaranteed. We answer this in the positive by showing that a class of ""counter-based algorithms"" (including the popular and very space-efficient FREQUENT and SPACESAVING algorithms) provide much stronger approximation guarantees than previously known. Specifically, we show that errors in the approximation of individual elements do not depend on the frequencies of the most frequent elements, but only on the frequency of the remaining ""tail."" This shows that counter-based methods are the most space-efficient (in fact, space-optimal) algorithms having this strong error bound. This tail guarantee allows these algorithms to solve the ""sparse recovery"" problem. Here, the goal is to recover a faithful representation of the vector of frequencies, f. We prove that using space O(k), the algorithms construct an approximation f* to the frequency vector f so that the L1 error ||f -- f*||1 is close to the best possible error minf2 ||f2 -- f||1, where f2 ranges over all vectors with at most k non-zero entries. This improves the previously best known space bound of about O(k log n) for streams without element deletions (where n is the size of the domain from which stream elements are drawn). Other consequences of the tail guarantees are results for skewed (Zipfian) data, and guarantees for accuracy of merging multiple summarized streams."
article,2908894,Sam  Cramer and Michael  Kampouridis and Alex  Freitas,,,,A Genetic Decomposition Algorithm for Predicting Rainfall Within Financial Weather Derivatives,885--892,,8,"decomposition, genetic programming, rainfall derivatives, rainfall prediction",10.1145/2908812.2908894,,,,,,,2016,,Proceedings of the Genetic and Evolutionary Computation Conference 2016,GECCO '16,,978-1-4503-4206-3,"Denver, Colorado, USA",ACM,"New York, NY, USA","Regression problems provide some of the most challenging research opportunities, where the predictions of such domains are critical to a specific application. Problem domains that exhibit large variability and are of chaotic nature are the most challenging to predict. Rainfall being a prime example, as it exhibits very unique characteristics that do not exist in other time series data. Moreover, rainfall is essential for applications that surround financial securities such as rainfall derivatives. This paper is interested in creating a new methodology for increasing the predictive accuracy of rainfall within the problem domain of rainfall derivatives. Currently, the process of predicting rainfall within rainfall derivatives is dominated by statistical models, namely Markov-chain extended with rainfall prediction (MCRP). In this paper, we propose a novel algorithm for decomposing rainfall, which is a hybrid Genetic Programming/Genetic Algorithm (GP/GA) algorithm. Hence, the overall problem becomes easier to solve. We compare the performance of our hybrid GP/GA, against MCRP, Radial Basis Function and GP without decomposition. We aim to show the effectiveness that a decomposition algorithm can have on the problem domain. Results show that in general decomposition has a very positive effect by statistically outperforming GP without decomposition and MCRP."
article,2627437,Manu  Nandan and Pramod P. Khargonekar and Sachin S. Talathi,,,,Fast SVM Training Using Approximate Extreme Points,59--98,,40,"convex hulls, extreme points, large scale classification, non-linear kernels, support vector machines",,J. Mach. Learn. Res.,January 2014,15,1,,January,2014,1532-4435,,,,,,JMLR.org,,"Applications of non-linear kernel support vector machines (SVMs) to large data sets is seriously hampered by its excessive training time. We propose a modification, called the approximate extreme points support vector machine (AESVM), that is aimed at overcoming this burden. Our approach relies on conducting the SVM optimization over a carefully selected subset, called the representative set, of the training data set. We present analytical results that indicate the similarity of AESVM and SVM solutions. A linear time algorithm based on convex hulls and extreme points is used to compute the representative set in kernel space. Extensive computational experiments on nine data sets compared AESVM to LIBSVM (Chang and Lin, 2011), CVM (Tsang et al., 2005), BVM (Tsang et al., 2007), LASVM (Bordes et al., 2005), SVMperf (Joachims and Yu, 2009), and the random features method (Rahimi and Recht, 2007). Our AESVM implementation was found to train much faster than the other methods, while its classification accuracy was similar to that of LIBSVM in all cases. In particular, for a seizure detection data set, AESVM training was almost 500 times faster than LIBSVM and LASVM and 20 times faster than CVM and BVM. Additionally, AESVM also gave competitively fast classification times."
article,3053473,Stijn  Luca and David A. Clifton and Bart  Vanrumste,,,,One-class Classification of Point Patterns of Extremes,6581--6601,,21,"asymptotic theory, class imbalance, extreme value theory, novelty detection, sequence classification",,J. Mach. Learn. Res.,January 2016,17,1,,January,2016,1532-4435,,,,,,JMLR.org,,"Novelty detection or one-class classification starts from a model describing some type of 'normal behaviour' and aims to classify deviations from this model as being either novelties or anomalies. In this paper the problem of novelty detection for point patterns S = {x1,...,xk} ⊂ Rd is treated where examples of anomalies are very sparse, or even absent. The latter complicates the tuning of hyperparameters in models commonly used for novelty detection, such as one-class support vector machines and hidden Markov models. To this end, the use of extreme value statistics is introduced to estimate explicitly a model for the abnormal class by means of extrapolation from a statistical model X for the normal class. We show how multiple types of information obtained from any available extreme instances of S can be combined to reduce the high false-alarm rate that is typically encountered when classes are strongly imbalanced, as often occurs in the one-class setting (whereby 'abnormal' data are often scarce). The approach is illustrated using simulated data and then a real-life application is used as an exemplar, whereby accelerometry data from epileptic seizures are analysed - these are known to be extreme and rare with respect to normal accelerometer data."
article,3056641,Songpon  Sastrawaha and Punyaphol  Horata,,,,Ensemble Extreme Learning Machine for Multi-instance Learning,56--60,,5,"Ensembles, Extreme Learning Machine, Majority voting, Multi-instance learning",10.1145/3055635.3056641,,,,,,,2017,,Proceedings of the 9th International Conference on Machine Learning and Computing,ICMLC 2017,,978-1-4503-4817-1,"Singapore, Singapore",ACM,"New York, NY, USA","Multi-instance learning (MIL) is a classification approach for classifying on a collection of instances which each group is represented as a bag. The main task of MIL is to learn from labels and features of instances to produce a model to predict a label of a testing bag. Traditional MIL algorithms were proposed to address the MIL problem, but most of the algorithms take a large time scale for their training process since they have to computing the parameter tuning. To address the learning time problem, the multi-instance learning method based on extreme learning machine (ELM-MIL) was proposed. However, the randomly generated parameters of ELM-MIL may reduce its generalization performance. Therefore, we proposed a new method to improve the generalization performance of the ELM-MIL which the new method is based on the ensemble with majority voting approach named the ensemble extreme learning machine for multi-instance learning (E-ELM-MIL). To evaluate the new method, several benchmark datasets were studied in this paper. From experimental results show that E-ELM-MIL outperforms ELM-MIL and the other state of the art MIL algorithm."
article,2978409,Zhan  Qin and Yin  Yang and Ting  Yu and Issa  Khalil and Xiaokui  Xiao and Kui  Ren,,,,Heavy Hitter Estimation over Set-Valued Data with Local Differential Privacy,192--203,,12,"heavy hitter, local differential privacy",10.1145/2976749.2978409,,,,,,,2016,,Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security,CCS '16,,978-1-4503-4139-4,"Vienna, Austria",ACM,"New York, NY, USA","In local differential privacy (LDP), each user perturbs her data locally before sending the noisy data to a data collector. The latter then analyzes the data to obtain useful statistics. Unlike the setting of centralized differential privacy, in LDP the data collector never gains access to the exact values of sensitive data, which protects not only the privacy of data contributors but also the collector itself against the risk of potential data leakage. Existing LDP solutions in the literature are mostly limited to the case that each user possesses a tuple of numeric or categorical values, and the data collector computes basic statistics such as counts or mean values. To the best of our knowledge, no existing work tackles more complex data mining tasks such as heavy hitter discovery over set-valued data. In this paper, we present a systematic study of heavy hitter mining under LDP. We first review existing solutions, extend them to the heavy hitter estimation, and explain why their effectiveness is limited. We then propose LDPMiner, a two-phase mechanism for obtaining accurate heavy hitters with LDP. The main idea is to first gather a candidate set of heavy hitters using a portion of the privacy budget, and focus the remaining budget on refining the candidate set in a second phase, which is much more efficient budget-wise than obtaining the heavy hitters directly from the whole dataset. We provide both in-depth theoretical analysis and extensive experiments to compare LDPMiner against adaptations of previous solutions. The results show that LDPMiner significantly improves over existing methods. More importantly, LDPMiner successfully identifies the majority true heavy hitters in practical settings."
article,3055374,Ruizhou  Ding and Dimitrios  Stamoulis and Kartikeya  Bhardwaj and Diana  Marculescu and Radu  Marculescu,,,,Enhancing Precipitation Models by Capturing Multivariate and Multiscale Climate Dynamics,39--42,,4,"carbon emissions, climate anomalies, complex network properties, complex networks, precipitation",10.1145/3055366.3055374,,,,,,,2017,,Proceedings of the 3rd International Workshop on Cyber-Physical Systems for Smart Water Networks,CySWATER '17,,978-1-4503-4975-8,"Pittsburgh, Pennsylvania",ACM,"New York, NY, USA","To improve precipitation predictions and enable accurate rainfall models for smart water networks, it is imperative to account for multivariate and multiscale precipitation dynamics with long-memory temporal relationships, long-range spatial dependencies, and low-frequency variability. While prior art has motivated the use of complex networks to capture these trends, existing work is limited to specific climate phenomena (such as El Niño) and regions. In this paper we employ a comprehensive assessment of complex networks with respect to multivariate dynamics across multiple temporal and spatial scales (multiscale). Our work is the first to incorporate both carbon emissions and precipitation anomalies data into a multivariate analysis. By effectively substantiating the ability of complex networks to capture multivariate and multiscale climate dynamics, we postulate their potential as reanalysis and assessment tools to enhance regional water models of arbitrary range and granularity, and to eventually enable reliable smart water systems."
article,2946663,Daniel  Hsu and Sivan  Sabato,,,,Loss Minimization and Parameter Estimation with Heavy Tails,543--582,,40,"heavy-tailed distributions, least squares, linear regression, unbounded losses",,J. Mach. Learn. Res.,January 2016,17,1,,January,2016,1532-4435,,,,,,JMLR.org,,"This work studies applications and generalizations of a simple estimation technique that provides exponential concentration under heavy-tailed distributions, assuming only bounded low-order moments. We show that the technique can be used for approximate minimization of smooth and strongly convex losses, and specifically for least squares linear regression. For instance, our d-dimensional estimator requires just O(d log(1/δ)) random samples to obtain a constant factor approximation to the optimal least squares loss with probability 1-δ, without requiring the covariates or noise to be bounded or subgaussian. We provide further applications to sparse linear regression and low-rank covariance matrix estimation with similar allowances on the noise and covariate distributions. The core technique is a generalization of the median-of-means estimator to arbitrary metric spaces."
article,2791468,K.  Namitha and A.  Jayapriya and G. Santhosh Kumar,,,,Rainfall Prediction Using Artificial Neural Network on Map-Reduce Framework,492--495,,4,"Artificial Neural Network, map-reduce, rainfall forecasting",10.1145/2791405.2791468,,,,,,,2015,,Proceedings of the Third International Symposium on Women in Computing and Informatics,WCI '15,,978-1-4503-3361-0,"Kochi, India",ACM,"New York, NY, USA","Big data is a celebrated topic in Business as well as research community for several years. With the revolution of Big Data, it is becoming easy and less expensive to store tremendous amount of data for future analysis. Weather data gets accumulated very fast and on a large scale. Thorough analysis and research is required on handling this big data and utilizing it for accurate weather prediction. As deterministic weather forecasting models are usually time consuming, it becomes challenging to efficiently use this large volume of data in hand. Machine learning methods are already proved to be good replacement for traditional deterministic approaches in weather prediction. These algorithms are popular for their scalability and hence more suitable in big data solutions. This paper proposes an approach of processing such Big volume of weather Data using Hadoop. Proposal includes Artificial Neural Network implemented on Map-reduce framework for short term rainfall prediction. Rainfall is predicted one day ahead using temperature and rainfall data of immediately preceding days. Temperature and Rainfall data of India over past 63 years (1951-2013) is used for this study."
article,1141985,Kshitiz  Garg and Shree K. Nayar,,,,Photorealistic Rendering of Rain Streaks,996--1002,,7,"oscillations, particle system, rain rendering, rain streak appearance, rain streak database, raindrops",10.1145/1141911.1141985,ACM Trans. Graph.,July 2006,25,3,,July,2006,0730-0301,,,,,,ACM,"New York, NY, USA","Photorealistic rendering of rain streaks with lighting and viewpoint effects is a challenging problem. Raindrops undergo rapid shape distortions as they fall, a phenomenon referred to as oscillations. Due to these oscillations, the reflection of light by, and the refraction of light through, a falling raindrop produce complex brightness patterns within a single motion-blurred rain streak captured by a camera or observed by a human. The brightness pattern of a rain streak typically includes speckles, multiple smeared highlights and curved brightness contours. In this work, we propose a new model for rain streak appearance that captures the complex interactions between the lighting direction, the viewing direction and the oscillating shape of the drop. Our model builds upon a raindrop oscillation model that has been developed in atmospheric sciences. We have measured rain streak appearances under a wide range of lighting and viewing conditions and empirically determined the oscillation parameters that are dominant in raindrops. Using these parameters, we have rendered thousands of rain streaks to create a database that captures the variations in streak appearance with respect to lighting and viewing directions. We have developed an efficient image-based rendering algorithm that uses our streak database to add rain to a single image or a captured video with moving objects and sources. The rendering algorithm is very simple to use as it only requires a coarse depth map of the scene and the locations and properties of the light sources. We have rendered rain in a wide range of scenarios and the results show that our physically-based rain streak model greatly enhances the visual realism of rendered rain."
article,1141985,Kshitiz  Garg and Shree K. Nayar,,,,Photorealistic Rendering of Rain Streaks,996--1002,,7,"oscillations, particle system, rain rendering, rain streak appearance, rain streak database, raindrops",10.1145/1179352.1141985,,,,,,,2006,,ACM SIGGRAPH 2006 Papers,SIGGRAPH '06,,1-59593-364-6,"Boston, Massachusetts",ACM,"New York, NY, USA","Photorealistic rendering of rain streaks with lighting and viewpoint effects is a challenging problem. Raindrops undergo rapid shape distortions as they fall, a phenomenon referred to as oscillations. Due to these oscillations, the reflection of light by, and the refraction of light through, a falling raindrop produce complex brightness patterns within a single motion-blurred rain streak captured by a camera or observed by a human. The brightness pattern of a rain streak typically includes speckles, multiple smeared highlights and curved brightness contours. In this work, we propose a new model for rain streak appearance that captures the complex interactions between the lighting direction, the viewing direction and the oscillating shape of the drop. Our model builds upon a raindrop oscillation model that has been developed in atmospheric sciences. We have measured rain streak appearances under a wide range of lighting and viewing conditions and empirically determined the oscillation parameters that are dominant in raindrops. Using these parameters, we have rendered thousands of rain streaks to create a database that captures the variations in streak appearance with respect to lighting and viewing directions. We have developed an efficient image-based rendering algorithm that uses our streak database to add rain to a single image or a captured video with moving objects and sources. The rendering algorithm is very simple to use as it only requires a coarse depth map of the scene and the locations and properties of the light sources. We have rendered rain in a wide range of scenarios and the results show that our physically-based rain streak model greatly enhances the visual realism of rendered rain."
article,2777082,Katsiaryna  Mirylenka and Graham  Cormode and Themis  Palpanas and Divesh  Srivastava,,,,Conditional Heavy Hitters: Detecting Interesting Correlations in Data Streams,395--414,,20,"Heavy hitters, Online algorithms, Streaming data",10.1007/s00778-015-0382-5,The VLDB Journal,June      2015,24,3,,June,2015,1066-8888,,,,,,"Springer-Verlag New York, Inc.","Secaucus, NJ, USA","The notion of heavy hitters--items that make up a large fraction of the population--has been successfully used in a variety of applications across sensor and RFID monitoring, network data analysis, event mining, and more. Yet this notion often fails to capture the semantics we desire when we observe data in the form of correlated pairs. Here, we are interested in items that are conditionally frequent: when a particular item is frequent within the context of its parent item. In this work, we introduce and formalize the notion of conditional heavy hitters to identify such items, with applications in network monitoring and Markov chain modeling. We explore the relationship between conditional heavy hitters and other related notions in the literature, and show analytically and experimentally the usefulness of our approach. We introduce several algorithm variations that allow us to efficiently find conditional heavy hitters for input data with very different characteristics, and provide analytical results for their performance. Finally, we perform experimental evaluations with several synthetic and real datasets to demonstrate the efficacy of our methods and to study the behavior of the proposed algorithms for different types of data."
article,2893184,Martin  Marinov and Nicholas  Nash and David  Gregg,,,,Practical Algorithms for Finding Extremal Sets,1.9:1--1.9:21,1.9,21,"Algorithms, dataset, extremal sets, itemset, memoization, parallel, practical",10.1145/2893184,J. Exp. Algorithmics,2016,21,,,April,2016,1084-6654,,,,,,ACM,"New York, NY, USA","The minimal sets within a collection of sets are defined as the ones that do not have a proper subset within the collection, and the maximal sets are the ones that do not have a proper superset within the collection. Identifying extremal sets is a fundamental problem with a wide range of applications in SAT solvers, data mining, and social network analysis. In this article, we present two novel improvements of the high-quality extremal set identification algorithm, AMS-Lex, described by Bayardo and Panda. The first technique uses memoization to improve the execution time of the single-threaded variant of the AMS-Lex, while our second improvement uses parallel programming methods. In a subset of the presented experiments, our memoized algorithm executes more than 400 times faster than the highly efficient publicly available implementation of AMS-Lex. Moreover, we show that our modified algorithm's speedup is not bounded above by a constant and that it increases as the length of the common prefixes in successive input itemsets increases. We provide experimental results using both real-world and synthetic datasets, and show our multithreaded variant algorithm outperforming AMS-Lex by 3 to 6 times. We find that on synthetic input datasets, when executed using 16 CPU cores of a 32-core machine, our multithreaded program executes about as fast as the state-of-the-art parallel GPU-based program using an NVIDIA GTX 580 graphics processing unit."
article,3076088,Adis  Alihodzic and Eva  Tuba and Milan  Tuba,,,,An Upgraded Bat Algorithm for Tuning Extreme Learning Machines for Data Classification,125--126,,2,"bat algorithm, extreme learning machine, swarm intelligence",10.1145/3067695.3076088,,,,,,,2017,,Proceedings of the Genetic and Evolutionary Computation Conference Companion,GECCO '17,,978-1-4503-4939-0,"Berlin, Germany",ACM,"New York, NY, USA","The learning time of the synaptic weights for feedforward neural networks tend to be very long. In order to reduce the learning time, in this paper we propose a new learning algorithm for learning the synaptic weights of the single-hidden-layer feedforward neural networks by combining the upgraded bat algorithm with the extreme learning machine. The proposed approach can efficiently search for the optimal input weights as well as the hidden biases, leading to the reduced number of evaluations needed to train a neural network. The experimental results based on classification problems and comparison with other approaches from literature have shown that the proposed algorithm produces a satisfactory performance in almost all cases and that it can learn the weight factors much faster than the traditional learning algorithms."
article,2820843,Ran  Wang and Chi-Yin  Chow and Yan  Lyu and Victor C. S. Lee and Sam  Kwong and Yanhua  Li and Jia  Zeng,,,,TaxiRec: Recommending Road Clusters to Taxi Drivers Using Ranking-based Extreme Learning Machines,53:1--53:4,53,4,"extreme learning machine, passenger-finding potential, recommender system, taxi trajectory data",10.1145/2820783.2820843,,,,,,,2015,,Proceedings of the 23rd SIGSPATIAL International Conference on Advances in Geographic Information Systems,SIGSPATIAL '15,,978-1-4503-3967-4,"Seattle, Washington",ACM,"New York, NY, USA","Utilizing large-scale GPS data to improve taxi services becomes a popular research problem in the areas of data mining, intelligent transportation, and the Internet of Things. In this paper, we utilize a large-scale GPS data set generated by over 7,000 taxis in a period of one month in Nanjing, China, and propose TaxiRec; a framework for discovering the passenger-finding potentials of road clusters, which is incorporated into a recommender system for taxi drivers to hunt passengers. In TaxiRec, we first construct the road network by defining the nodes and road segments. Then, the road network is divided into a number of road clusters through a clustering process on the mid points of the road segments. Afterwards, a set of features for each road cluster is extracted from real-life data sets, and a ranking-based extreme learning machine (ELM) model is proposed to evaluate the passenger-finding potential of each road cluster. Experimental results demonstrate the feasibility and effectiveness of the proposed framework."
article,2701177,Saima  Hassan and Abbas  Khosravi and Jafreezal  Jaafar,,,,Training of Interval Type-2 Fuzzy Logic System Using Extreme Learning Machine for Load Forecasting,87:1--87:5,87,5,"extreme learning machine, interval type-2 fuzzy logic systems, learning algorithm, load forecasting, parameter optimization",10.1145/2701126.2701177,,,,,,,2015,,Proceedings of the 9th International Conference on Ubiquitous Information Management and Communication,IMCOM '15,,978-1-4503-3377-1,"Bali, Indonesia",ACM,"New York, NY, USA","Extreme learning machine (ELM) is originally proposed for single-hidden layer feed-forward neural networks (SLFN). From the functional equivalence of fuzzy logic systems and SLFN, the fuzzy logic systems can be interpreted as a special case of SLFN under some mild conditions. Hence the fuzzy logic systems can be trained using SLFN's learning algorithms. Considering the same equivalence, ELM is utilized here to train interval type-2 fuzzy logic systems (IT2FLSs). Based on the working principle of the ELM, the parameters of the antecedent of IT2FLSs are randomly generated while the consequent part of IT2FLSs is optimized using Moore-Penrose generalized inverse of ELM. Application of the developed model to electricity load forecasting is another novelty of the research work. Experimental results shows better forecasting performance of the proposed model over the two frequently used forecasting models."
article,3054204,Yukihiro  Tagami,,,,Learning Extreme Multi-label Tree-classifier via Nearest Neighbor Graph Partitioning,845--846,,2,"approximate k-nearest neighbor graph, extreme multi-label classification",10.1145/3041021.3054204,,,,,,,2017,,Proceedings of the 26th International Conference on World Wide Web Companion,WWW '17 Companion,,978-1-4503-4914-7,"Perth, Australia",International World Wide Web Conferences Steering Committee,"Republic and Canton of Geneva, Switzerland","Web scale classification problems, such as Web page tagging and E-commerce product recommendation, are typically regarded as multi-label classification with an extremely large number of labels. In this paper, we propose GPT, which is a novel tree-based approach for extreme multi-label learning. GPT recursively splits a feature space with a hyperplane at each internal node, considering approximate k-nearest neighbor graph on the label space. We learn the linear binary classifiers using a simple optimization procedure. We conducted evaluations on several large-scale real-world data sets and compared our proposed method with recent state-of-the-art methods. Experimental results demonstrate the effectiveness of our proposed method."
article,1632388,Wanyu  Deng and Qinghua  Zheng and Lin  Chen,,,,Real-Time Collaborative Filtering Using Extreme Learning Machine,466--473,,8,"collaborative filtering, Extreme Learning Machine",10.1109/WI-IAT.2009.80,,,,,,,2009,,Proceedings of the 2009 IEEE/WIC/ACM International Joint Conference on Web Intelligence and Intelligent Agent Technology - Volume 01,WI-IAT '09,,978-0-7695-3801-3,,IEEE Computer Society,"Washington, DC, USA","Because of long-consuming training or similarity computing, most traditional collaborative filtering algorithms are off-line methods and can't be applied in collaborative-filtering services that have accumulated large amounts of data and need to compute predictions under real-time conditions. In order to address this problem, we propose a novel real-time collaborative filtering algorithm, called RCF, based on Extreme Learning Machine (ELM). The initial training and updating of RCF are very fast and can be finished in real time. The experimental results show that the mean recommendation time of RCF is shorter than SVD/ANN and correlation-based algorithms reported in other papers while the accuracy is better."
article,3080834,Jingzhou  Liu and Wei-Cheng  Chang and Yuexin  Wu and Yiming  Yang,,,,Deep Learning for Extreme Multi-label Text Classification,115--124,,10,"convolutional neural network, deep learning, extreme text classification, multi-label",10.1145/3077136.3080834,,,,,,,2017,,Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval,SIGIR '17,,978-1-4503-5022-8,"Shinjuku, Tokyo, Japan",ACM,"New York, NY, USA","Extreme multi-label text classification (XMTC) refers to the problem of assigning to each document its most relevant subset of class labels from an extremely large label collection, where the number of labels could reach hundreds of thousands or millions. The huge label space raises research challenges such as data sparsity and scalability. Significant progress has been made in recent years by the development of new machine learning methods, such as tree induction with large-margin partitions of the instance spaces and label-vector embedding in the target space. However, deep learning has not been explored for XMTC, despite its big successes in other related areas. This paper presents the first attempt at applying deep learning to XMTC, with a family of new Convolutional Neural Network (CNN) models which are tailored for multi-label classification in particular. With a comparative evaluation of 7 state-of-the-art methods on 6 benchmark datasets where the number of labels is up to 670,000, we show that the proposed CNN approach successfully scaled to the largest datasets, and consistently produced the best or the second best results on all the datasets. On the Wikipedia dataset with over 2 million documents and 500,000 labels in particular, it outperformed the second best method by 11.7%~15.3% in [email protected] and by 11.5%~11.7% in [email protected] for K = 1,3,5."
article,1299038,Runxuan  Zhang and Guang-Bin  Huang and N.  Sundararajan and P.  Saratchandran,,,,Multicategory Classification Using An Extreme Learning Machine for Microarray Gene Expression Cancer Diagnosis,485--495,,11,"Extreme learning machine, gene expression, microarray, multi-category classification, SVM",10.1109/tcbb.2007.1012,IEEE/ACM Trans. Comput. Biol. Bioinformatics,July 2007,4,3,,July,2007,1545-5963,,,,,,IEEE Computer Society Press,"Los Alamitos, CA, USA","In this paper, the recently developed Extreme Learning Machine (ELM) is used for direct multicategory classification problems in the cancer diagnosis area. ELM avoids problems like local minima, improper learning rate and overfitting commonly faced by iterative learning methods and completes the training very fast. We have evaluated the multi-category classification performance of ELM on three benchmark microarray datasets for cancer diagnosis, namely, the GCM dataset, the Lung dataset and the Lymphoma dataset. The results indicate that ELM produces comparable or better classification accuracies with reduced training time and implementation complexity compared to artificial neural networks methods like conventional back-propagation ANN, Linder's SANN, and Support Vector Machine methods like SVM-OVO and Ramaswamy's SVM-OVA. ELM also achieves better accuracies for classification of individual categories."
article,2942492,Weina  Wang and Kai  Zhu and Lei  Ying and Jian  Tan and Li  Zhang,,,,MapTask Scheduling in Mapreduce with Data Locality: Throughput and Heavy-traffic Optimality,190--203,,14,"MapReduce, heavy-traffic analysis, queueing systems, scheduling, throughput optimality",10.1109/TNET.2014.2362745,IEEE/ACM Trans. Netw.,February 2016,24,1,,February,2016,1063-6692,,,,,,IEEE Press,"Piscataway, NJ, USA","MapReduce/Hadoop framework has been widely used to process large-scale datasets on computing clusters. Scheduling map tasks with data locality consideration is crucial to the performance of MapReduce. Many works have been devoted to increasing data locality for better efficiency. However, to the best of our knowledge, fundamental limits of MapReduce computing clusters with data locality, including the capacity region and theoretical bounds on the delay performance, have not been well studied. In this paper, we address these problems from a stochastic network perspective. Our focus is to strike the right balance between data locality and load balancing to simultaneously maximize throughput and minimize delay. We present a new queueing architecture and propose a map task scheduling algorithm constituted by the Join the Shortest Queue policy together with the MaxWeight policy. We identify an outer bound on the capacity region, and then prove that the proposed algorithm can stabilize any arrival rate vector strictly within this outer bound. It shows that the outer bound coincides with the actual capacity region, and the proposed algorithm is throughput-optimal. Furthermore, we study the number of backlogged tasks under the proposed algorithm, which is directly related to the delay performance based on Little's law. We prove that the proposed algorithm is heavy-traffic optimal, i.e., it asymptotically minimizes the number of back-logged tasks as the arrival rate vector approaches the boundary of the capacity region. Therefore, the proposed algorithm is also delay-optimal in the heavy-traffic regime. The proofs in this paper deal with random processing times with heterogeneous parameters and nonpreemptive task execution, which differentiate our work from many existing works on MaxWeight-type algorithms, so the proof techniques themselves for the stability analysis and the heavy-traffic analysis are also novel contributions."
article,1341433,Xenofontas  Dimitropoulos and Paul  Hurley and Andreas  Kind,,,,Probabilistic Lossy Counting: An Efficient Algorithm for Finding Heavy Hitters,5--5,,1,"data streams, heavy hitters",10.1145/1341431.1341433,SIGCOMM Comput. Commun. Rev.,January 2008,38,1,,January,2008,0146-4833,,,,,,ACM,"New York, NY, USA","Knowledge of the largest traffic ows in a network is important for many network management applications. The problem of finding these ows is known as the heavy-hitter problem and has been the subject of many studies in the past years. One of the most efficient and well-known algorithms for finding heavy hitters is lossy counting [29]. In this work we introduce probabilistic lossy counting (PLC), which enhances lossy counting in computing network traffic heavy hitters. PLC uses on a tighter error bound on the estimated sizes of traffic ows and provides probabilistic rather than deterministic guarantees on its accuracy. The probabilistic-based error bound substantially improves the memory consumption of the algorithm. In addition, PLC reduces the rate of false positives of lossy counting and achieves a low estimation error, although slightly higher than that of lossy counting We compare PLC with state-of-the-art algorithms for finding heavy hitters. Our experiments using real traffic traces find that PLC has 1) between 34.4% and 74% lower memory consumption, 2) between 37.9% and 40.5% fewer false positives than lossy counting, and 3) a small estimation error."
article,2666664,Junlong  Xiang and Magnus  Westerlund and Du&#353;an  Sovilj and G&#246;ran  Pulkkis,,,,Using Extreme Learning Machine for Intrusion Detection in a Big Data Environment,73--82,,10,"big data, classification, extreme learning machine, intrusion detection, mapreduce",10.1145/2666652.2666664,,,,,,,2014,,Proceedings of the 2014 Workshop on Artificial Intelligent and Security Workshop,AISec '14,,978-1-4503-3153-1,"Scottsdale, Arizona, USA",ACM,"New York, NY, USA","Extending state-of-the-art machine learning algorithms to highly scalable (big data) analysis environments is crucial for the handling of authentic datasets in Intrusion Detection Systems (IDS). Traditional supervised learning methods are considered to be too slow for use in these environments. Therefore, we propose the use of Extreme Learning Machine (ELM) for detecting network intrusion attempts. We show they hold great promise for the field by employing a MapReduce based variant evaluated on the open source tool Hadoop."
article,3060606,Diana Andreea Popescu and Gianni  Antichi and Andrew W. Moore,,,,Enabling Fast Hierarchical Heavy Hitter Detection Using Programmable Data Planes,191--192,,2,"Hierarchical Heavy Hitters, P4, SDN",10.1145/3050220.3060606,,,,,,,2017,,Proceedings of the Symposium on SDN Research,SOSR '17,,978-1-4503-4947-5,"Santa Clara, CA, USA",ACM,"New York, NY, USA","Measuring and monitoring network traffic is a fundamental aspect in network management. This poster is a first step towards an SDN solution using an event triggered approach to support advanced monitoring dataplane capabilities. Leveraging P4 programmability, we built a solution to inform a remote controller about the detected hierarchical heavy hitters, thus minimizing control plane overheads."
article,3140040,Monidipa  Das and Soumya K. Ghosh,,,,BESTED: An Exponentially Smoothed Spatial Bayesian Analysis Model for Spatio-temporal Prediction of Daily Precipitation,55:1--55:4,55,4,"Exponential smoothing, Precipitation, Spatial Bayesian network, Spatio-temporal prediction, Time series",10.1145/3139958.3140040,,,,,,,2017,,Proceedings of the 25th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems,SIGSPATIAL'17,,978-1-4503-5490-5,"Redondo Beach, CA, USA",ACM,"New York, NY, USA","This paper proposes a novel data-driven model (BESTED), based on spatial Bayesian network with incorporated exponential smoothing mechanism, for predicting precipitation time series on daily basis. In BESTED, the spatial Bayesian network helps to efficiently model the influence of spatially distributed variables. Moreover, the incorporated exponential smoothing mechanism aids in tuning the network inferred values to compensate for the unknown factors, influencing the precipitation rate. Empirical study has been carried out to predict the daily precipitation in West Bengal, India, for the year 2015. The experimental result demonstrates the superiority of the proposed BESTED model, compared to the other benchmarks and state-of-the-art techniques."
article,3018053,Sivanagaraja  Tatinati and Yubo  Wang and Kalyana C. Veluvolu,,,,Modeling of Physiological Tremor with Quaternion Variant of Extreme Learning Machines,255--258,,4,"extreme learning machines, modeling, physiological tremor, quaternion",10.1145/3018009.3018053,,,,,,,2016,,Proceedings of the 2Nd International Conference on Communication and Information Processing,ICCIP '16,,978-1-4503-4819-5,"Singapore, Singapore",ACM,"New York, NY, USA","Hand-held robotic surgical instruments are developed to acquire the maneuvered hand motion of the surgeon and then provide a control signal for real-time compensation of the physiological tremor in three-dimensional (3-D) space. For active tremor compensation, accurate modeling and estimation of physiological tremor is essential. The current modeling techniques that models tremor in 3D space consider the motion in three-axes [x, y, and z axes) as three separate one-dimensional signals and then perform modeling separately. Recently, it has been shown that for physiological tremor motion there exists cross dimensional coupling and it improves the modeling accuracy. Motivated by this, a quaternion variant for extreme learning machines is developed for accurate 3D modeling of tremor. The developed method is validated with real tremor data and the obtained results highlighted the suitability of this method for accurate tremor modeling in 3D space."
article,3064945,Anton  Akusok and Emil  Eirola and Kaj-Mikael  Bj&#246;rk and Yoan  Miche and Hans  Johnson and Amaury  Lendasse,,,,Brute-force Missing Data Extreme Learning Machine for Predicting Huntington's Disease,189--192,,4,"Extreme learning machine, Huntington's disease, imputation, missing values",10.1145/3056540.3064945,,,,,,,2017,,Proceedings of the 10th International Conference on PErvasive Technologies Related to Assistive Environments,PETRA '17,,978-1-4503-5227-7,"Island of Rhodes, Greece",ACM,"New York, NY, USA","This paper presents a novel procedure to train Extreme Learning Machine models on datasets with missing values. In effect, a separate model is learned to classify every sample in the test set, however, this is accomplished in an efficient manner which does not require accessing the training data repeatedly. Instead, a sparse structure is imposed on the input layer weights, which enables calculating the necessary statistics in the training phase. An application to predicting the progression of Huntington's disease from brain scans is presented. Experimental comparisons show promising results equivalent to the state of the art in machine learning with incomplete data."
article,1463528,Yong  Liu and David J. Hill and Alejandro  Rodriguez and Luigi  Marini and Rob  Kooper and Joe  Futrelle and Barbara  Minsker and James D. Myers,,,,Near-real-time Precipitation Virtual Sensor Using NEXRAD Data,82:1--82:2,82,2,"NEXRAD, digital watershed, ontology, precipitation, spatiotemporal and thematic transformation, virtual sensor, workflow",10.1145/1463434.1463528,,,,,,,2008,,Proceedings of the 16th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems,GIS '08,,978-1-60558-323-5,"Irvine, California",ACM,"New York, NY, USA","In this demonstration paper, we describe the technologies and implementations that allow near real-time creation of new virtual precipitation sensors using NEXRAD Level II streaming data at user-specified point locations and time intervals in an integrated digital watershed with a Google Map-based web interface. The spatiotemporal and thematic transformation steps to produce such new time series data stream are implemented as a set of scientific workflows. A streaming data ontology is developed to handle temporal proximity concepts such as ""previous"" and ""next"" for irregular temporal data streams. Data and metadata management is provided by a semantic content management middleware. The new point-based virtual sensor can lower the barriers of using NEXRAD data for many hydrological applications."
article,3126776,Stevan  Rudinac and Iva  Gornishka and Marcel  Worring,,,,Multimodal Classification of Violent Online Political Extremism Content with Graph Convolutional Networks,245--252,,8,"entity linking, graph convolutional networks, multimedia classification, semantic concepts, violent online political extremism",10.1145/3126686.3126776,,,,,,,2017,,Proceedings of the on Thematic Workshops of ACM Multimedia 2017,Thematic Workshops '17,,978-1-4503-5416-5,"Mountain View, California, USA",ACM,"New York, NY, USA","In this paper we present a multimodal approach to categorizing user posts based on their discussion topic. To integrate heterogeneous information extracted from the posts, i.e. text, visual content and the information about user interactions with the online platform, we deploy graph convolutional networks that were recently proven effective in classification tasks on knowledge graphs. As the case study we use the analysis of violent online political extremism content, a challenging task due to a particularly high semantic level at which extremist ideas are discussed. Here we demonstrate the potential of using neural networks on graphs for classifying multimedia content and, perhaps more importantly, the effectiveness of multimedia analysis techniques in aiding the domain experts performing qualitative data analysis. Our conclusions are supported by extensive experiments on a large collection of extremist posts."
article,1559820,Ke  Yi and Qin  Zhang,,,,Optimal Tracking of Distributed Heavy Hitters and Quantiles,167--174,,8,"distributed tracking, heavy hitter, quantile",10.1145/1559795.1559820,,,,,,,2009,,Proceedings of the Twenty-eighth ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems,PODS '09,,978-1-60558-553-6,"Providence, Rhode Island, USA",ACM,"New York, NY, USA","We consider the the problem of tracking heavy hitters and quantiles in the distributed streaming model. The heavy hitters and quantiles are two important statistics for characterizing a data distribution. Let A be a multiset of elements, drawn from the universe U={1,...,u}. For a given 0 ≤ Φ ≤ 1, the Φ-heavy hitters are those elements of A whose frequency in A is at least Φ |A|; the Φ-quantile of A is an element x of U such that at most Φ|A| elements of A are smaller than A and at most (1-Φ)|A| elements of A are greater than x. Suppose the elements of A are received at k remote sites over time, and each of the sites has a two-way communication channel to a designated coordinator, whose goal is to track the set of Φ-heavy hitters and the Φ-quantile of A approximately at all times with minimum communication. We give tracking algorithms with worst-case communication cost O(k/ε ⋅ log n) for both problems, where n is the total number of items in A, and ε is the approximation error. This substantially improves upon the previous known algorithms. We also give matching lower bounds on the communication costs for both problems, showing that our algorithms are optimal. We also consider a more general version of the problem where we simultaneously track the Φ-quantiles for all 0 ≤ Φ ≤ 1."
article,2369192,Krishna  Jagannathan and Mihalis  Markakis and Eytan  Modiano and John N. Tsitsiklis,,,,Queue-length Asymptotics for Generalized Max-weight Scheduling in the Presence of Heavy-tailed Traffic,1096--1111,,16,"heavy-tailed traffic, scheduling, throughput optimality",10.1109/TNET.2011.2173553,IEEE/ACM Trans. Netw.,August 2012,20,4,,August,2012,1063-6692,,,,,,IEEE Press,"Piscataway, NJ, USA","We investigate the asymptotic behavior of the steady-state queue-length distribution under generalized max-weight scheduling in the presence of heavy-tailed traffic. We consider a system consisting of two parallel queues, served by a single server. One of the queues receives heavy-tailed traffic, and the other receives light-tailed traffic. We study the class of throughput-optimal max-weight-α scheduling policies and derive an exact asymptotic characterization of the steady-state queue-length distributions. In particular, we show that the tail of the light queue distribution is at least as heavy as a power-law curve, whose tail coefficient we obtain explicitly. Our asymptotic characterization also shows that the celebrated max-weight scheduling policy leads to the worst possible tail coefficient of the light queue distribution, among all nonidling policies. Motivated by the above negative result regarding the max-weight-α policy, we analyze a log-max-weight (LMW) scheduling policy. We show that the LMWpolicy guarantees an exponentially decaying light queue tail while still being throughput-optimal."
article,3098832,Ran  Ben Basat and Gil  Einziger and Roy  Friedman and Marcelo C. Luizelli and Erez  Waisbard,,,,Constant Time Updates in Hierarchical Heavy Hitters,127--140,,14,"Heavy Hitters, Measurement, Monitoring, Streaming",10.1145/3098822.3098832,,,,,,,2017,,Proceedings of the Conference of the ACM Special Interest Group on Data Communication,SIGCOMM '17,,978-1-4503-4653-5,"Los Angeles, CA, USA",ACM,"New York, NY, USA","Monitoring tasks, such as anomaly and DDoS detection, require identifying frequent flow aggregates based on common IP prefixes. These are known as hierarchical heavy hitters (HHH), where the hierarchy is determined based on the type of prefixes of interest in a given application. The per packet complexity of existing HHH algorithms is proportional to the size of the hierarchy, imposing significant overheads. In this paper, we propose a randomized constant time algorithm for HHH. We prove probabilistic precision bounds backed by an empirical evaluation. Using four real Internet packet traces, we demonstrate that our algorithm indeed obtains comparable accuracy and recall as previous works, while running up to 62 times faster. Finally, we extended Open vSwitch (OVS) with our algorithm and showed it is able to handle 13.8 million packets per second. In contrast, incorporating previous works in OVS only obtained 2.5 times lower throughput."
article,2897558,Vladimir  Braverman and Stephen R.  Chestnut and Nikita  Ivkin and David P.  Woodruff,,,,Beating CountSketch for Heavy Hitters in Insertion Streams,740--753,,14,"Chaining, Data Streams, Heavy Hitters",10.1145/2897518.2897558,,,,,,,2016,,Proceedings of the Forty-eighth Annual ACM Symposium on Theory of Computing,STOC '16,,978-1-4503-4132-5,"Cambridge, MA, USA",ACM,"New York, NY, USA","Given a stream p1, …, pm of items from a universe U, which, without loss of generality we identify with the set of integers {1, 2, …, n}, we consider the problem of returning all ℓ2-heavy hitters, i.e., those items j for which fj ≥ є √F2, where fj is the number of occurrences of item j in the stream, and F2 = ∑i ∈ [n] fi2. Such a guarantee is considerably stronger than the ℓ1-guarantee, which finds those j for which fj ≥ є m. In 2002, Charikar, Chen, and Farach-Colton suggested the CountSketch data structure, which finds all such j using Θ(log2 n) bits of space (for constant є > 0). The only known lower bound is Ω(logn) bits of space, which comes from the need to specify the identities of the items found. In this paper we show one can achieve O(logn loglogn) bits of space for this problem. Our techniques, based on Gaussian processes, lead to a number of other new results for data streams, including: (1) The first algorithm for estimating F2 simultaneously at all points in a stream using only O(lognloglogn) bits of space, improving a natural union bound. (2) A way to estimate the ℓ∞ norm of a stream up to additive error є √F2 with O(lognloglogn) bits of space, resolving Open Question 3 from the IITK 2006 list for insertion only streams."
article,2666273,Heysem  Kaya and Albert Ali  Salah,,,,Combining Modality-Specific Extreme Learning Machines for Emotion Recognition in the Wild,487--493,,7,"audio-visual emotion corpus, audio-visual fusion, emotion recognition in the wild, extreme learning machines, feature extraction",10.1145/2663204.2666273,,,,,,,2014,,Proceedings of the 16th International Conference on Multimodal Interaction,ICMI '14,,978-1-4503-2885-2,"Istanbul, Turkey",ACM,"New York, NY, USA","This paper presents our contribution to ACM ICMI 2014 Emotion Recognition in the Wild Challenge and Workshop. The proposed system utilizes Extreme Learning Machines (ELM) for modeling modality-specific features and combines the scores for final prediction. The state-of-the-art results in acoustic and visual emotion recognition are obtained either using deep Neural Networks (DNN) or Support Vector Machines (SVM). The ELM paradigm is proposed as a fast and accurate alternative to these two popular machine learning methods. Benefiting from fast learning advantage of ELM, we carry out extensive tests on the data using moderate computational resources. In the video modality, we test combination of regional visual features obtained from the inner face. In the audio modality, we carry out tests to enhance training via other emotional corpora. We further investigate the suitability of several recently proposed feature selection approaches to prune the acoustic features. In our study, the best results for both modalities are obtained with Kernel ELM compared to basic ELM. On the challenge test set, we obtain 37.84%, 39.07% and 44.23% classification accuracies for audio, video and multimodal fusion, respectively."
article,2902284,Arnab  Bhattacharyya and Palash  Dey and David P. Woodruff,,,,An Optimal Algorithm for L1-Heavy Hitters in Insertion Streams and Related Problems,385--400,,16,"algorithms, data streams, frequent items, heavy hitters",10.1145/2902251.2902284,,,,,,,2016,,Proceedings of the 35th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems,PODS '16,,978-1-4503-4191-2,"San Francisco, California, USA",ACM,"New York, NY, USA","We give the first optimal bounds for returning the l1-heavy hitters in a data stream of insertions, together with their approximate frequencies, closing a long line of work on this problem. For a stream of m items in {1, 2, ..., n} and parameters 0 < ε < φ ≤ 1, let fi denote the frequency of item i, i.e., the number of times item i occurs in the stream. With arbitrarily large constant probability, our algorithm returns all items i for which fi ≥ φ m, returns no items j for which fj ≤ (φ -ε)m, and returns approximations ~fi with |~fi - fi| ≤ ε m for each item i that it returns. Our algorithm uses O(ε-1 logφ-1 + φ-1 log n + log log m) bits of space, processes each stream update in O(1) worst-case time, and can report its output in time linear in the output size. We also prove a lower bound, which implies that our algorithm is optimal up to a constant factor in its space complexity. A modification of our algorithm can be used to estimate the maximum frequency up to an additive ε m error in the above amount of space, resolving Question 3 in the IITK 2006 Workshop on Algorithms for Data Streams for the case of l1-heavy hitters. We also introduce several variants of the heavy hitters and maximum frequency problems, inspired by rank aggregation and voting schemes, and show how our techniques can be applied in such settings. Unlike the traditional heavy hitters problem, some of these variants look at comparisons between items rather than numerical values to determine the frequency of an item."
article,3131377,Kenjiro  Cho,,,,Recursive Lattice Search: Hierarchical Heavy Hitters Revisited,283--289,,7,"Z-order, flow aggregation algorithm, hierarchical heavy hitters",10.1145/3131365.3131377,,,,,,,2017,,Proceedings of the 2017 Internet Measurement Conference,IMC '17,,978-1-4503-5118-8,"London, United Kingdom",ACM,"New York, NY, USA","The multidimensional Hierarchical Heavy Hitter (HHH) problem identifies significant clusters in traffic across multiple planes such as source and destination addresses, and has been widely studied in the literature. A compact summary of HHHs provides an overview on complex traffic behavior and is a powerful means for traffic monitoring and anomaly detection. In this paper, we present a new efficient HHH algorithm which fits operational needs. Our key insight is to revisit the commonly accepted definition of HHH, and apply the Z-ordering to make use of a recursive partitioning algorithm. The proposed algorithm produces summary outputs comparable to or even better in practice than the existing algorithms, and runs orders of magnitude faster for bitwise aggregation. We have implemented the algorithm into our open-source tool and have made longitudinal datasets of backbone traffic openly available."
article,1530757,Yung  Yi and Junshan  Zhang and Mung  Chiang,,,,Delay and Effective Throughput of Wireless Scheduling in Heavy Traffic Regimes: Vacation Model for Complexity,55--64,,10,"complexity, heavy-traffic, scheduling, wireless networks",10.1145/1530748.1530757,,,,,,,2009,,Proceedings of the Tenth ACM International Symposium on Mobile Ad Hoc Networking and Computing,MobiHoc '09,,978-1-60558-624-3,"New Orleans, LA, USA",ACM,"New York, NY, USA","Distributed scheduling algorithms for wireless ad hoc networks have received substantial attention over the last decade. The complexity levels of these algorithms span a wide spectrum, ranging from no message passing to constant/polynomial time complexity, or even exponential complexity. However, by and large it remains open to quantify the impact of message passing complexity on throughput and delay. In this paper, we study the effective throughput and delay performance in wireless scheduling by explicitly considering complexity through a vacation model, where signaling complexity is treated as ""vacations"" and data transmissions as ""services,"" with a focus on delay analysis in heavy traffic regimes. We analyze delay performance in two regimes of vacation models, depending on the relative lengths of data transmission and vacation periods. State space collapse properties proved here enable a significant dimensionality reduction in the challenging problem of delay characterization. We then explore engineering implications and quantify intuitions based on the heavy traffic analysis."
article,2927977,Da  Tong and Viktor  Prasanna,,,,High Throughput Sketch Based Online Heavy Hitter Detection on FPGA,70--75,,6,"Heavy hitter detection, Sketch data structure, Streaming algorithm",10.1145/2927964.2927977,SIGARCH Comput. Archit. News,September 2015,43,4,,April,2016,0163-5964,,,,,,ACM,"New York, NY, USA","In the context of networking, a heavy hitter is an entity in a data stream whose amount of activity (such as bandwidth consumption or number of connections) is higher than a given threshold. Detecting heavy hitters is a critical task for network management and security in the Internet and data centers. Data streams in modern network usually contain millions of entities, such as traffic flows or IP domains. It is challenging to detect heavy hitters at a high throughput while supporting such a large number of entities. I this work, we propose a high throughput online heavy hitter detector based on the Count-min sketch algorithm on FPGA. We propose a high throughput hash computation architecture, optimize the Count-min sketch for hardwarebased heavy hitter detection and use forwarding to deal with data hazards. The post place-and-route results of our architecture on a state-of-the-art FPGA shows high throughput and scalability. Our architecture achieves a throughput of 114 Gbps while supporting a typical 1 M concurrent entities. It sustains 100+ Gbps throughput while supporting various number of concurrent entities, stream sizes and accuracy requirements. Our implementation demonstrates improved performance compared with other sketch acceleration techniques on various platforms using similar sketch configurations."
article,640103,Wentian  Li and Ivo  Grosse,,,,Gene Selection Criterion for Discriminant Microarray Data Analysis Based on Extreme Value Distributions,217--223,,7,"classification, extreme values, logistic regression, microarray",10.1145/640075.640103,,,,,,,2003,,Proceedings of the Seventh Annual International Conference on Research in Computational Molecular Biology,RECOMB '03,,1-58113-635-8,"Berlin, Germany",ACM,"New York, NY, USA","An important issue commonly encountered in the analysis of microarray data is to decide which and how many genes should be selected for further studies. For discriminant microarray data analyses based on statistical models, such as the logistic regression model, this gene selection can be accomplished by a comparison of the maximum likelihood of the model given the real data, L(D|M), and the expected maximum likelihood of the model given an ensemble of surrogate data, L(D0|M). Typically, the computational burden for obtaining L(D0|M) is immense, often exceeding the limits of available resources by orders of magnitude. Here, we propose an approach that circumvents such heavy computations by mapping the simulation problem to an extreme value problem, which can be easily solved by numerical simulation. We choose three classification problems from two publicly available microarray datasets to illustrate that approach."
article,3034798,Vladimir  Braverman and Stephen R. Chestnut and Nikita  Ivkin and Jelani  Nelson and Zhengyu  Wang and David P. Woodruff,,,,BPTree: An &#8467;2 Heavy Hitters Algorithm Using Constant Memory,361--376,,16,"bernoulli processes, chaining, data streams, frequent items, heavy hitters",10.1145/3034786.3034798,,,,,,,2017,,Proceedings of the 36th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems,PODS '17,,978-1-4503-4198-1,"Chicago, Illinois, USA",ACM,"New York, NY, USA","The task of finding heavy hitters is one of the best known and well studied problems in the area of data streams. One is given a list i1,i2,...,im∈[n] and the goal is to identify the items among [n] that appear frequently in the list. In sub-polynomial space, the strongest guarantee available is the l2 guarantee, which requires finding all items that occur at least ε||ƒ||2 times in the stream, where the vector ƒ∈Rn is the count histogram of the stream with ith coordinate equal to the number of times i appears ƒi:=#{jε[m]:ij=i. The first algorithm to achieve the l2 guarantee was the CountSketch of [11], which requires O(ε-2log n) words of memory and O(log n) update time and is known to be space-optimal if the stream allows for deletions. The recent work of [7] gave an improved algorithm for insertion-only streams, using only O(ε-2logε-1log log n) words of memory. In this work, we give an algorithm BPTree for l2 heavy hitters in insertion-only streams that achieves O(ε-2logε-1) words of memory and O(logε-1) update time, which is the optimal dependence on n and m. In addition, we describe an algorithm for tracking ||ƒ||2 at all times with O(ε-2) memory and update time. Our analyses rely on bounding the expected supremum of a Bernoulli process involving Rademachers with limited independence, which we accomplish via a Dudley-like chaining argument that may have applications elsewhere."
article,2491304,Bin  Li and Ruogu  Li and Atilla  Eryilmaz,,,,Heavy-traffic-optimal Scheduling with Regular Service Guarantees in Wireless Networks,79--88,,10,"heavy-traffic analysis, mean delay, service regularity, throughput, wireless scheduling",10.1145/2491288.2491304,,,,,,,2013,,Proceedings of the Fourteenth ACM International Symposium on Mobile Ad Hoc Networking and Computing,MobiHoc '13,,978-1-4503-2193-8,"Bangalore, India",ACM,"New York, NY, USA","We consider the design of throughput-optimal scheduling policies in multi-hop wireless networks that also possess good mean delay performance and provide regular service for all links -- critical metrics for real-time applications. To that end, we study a parametric class of maximum-weight type scheduling policies with parameter α ≥ 0, called Regular Service Guarantee (RSG) Algorithm, where each link weight consists of its own queue-length and a counter that tracks the time since the last service. This policy has been shown to be throughput-optimal and to provide more regular service as the parameter α increases, however at the cost of increasing mean delay. This motivates us to investigate whether satisfactory service regularity and low mean-delay can be simultaneously achieved by the RSG Algorithm by carefully selecting its parameter α. To that end, we perform a novel Lyapunov-drift based analysis of the steady-state behavior of the stochastic network. Our analysis reveals that the RSG Algorithm can minimize the total mean queue-length to establish mean delay optimality under heavily-loaded conditions as long as α scales no faster than the order of 1/5√∈, where ∈ measures the closeness of the network load to the boundary of the capacity region. To the best of our knowledge, this is the first work that provides regular service to all links while also achieving heavy-traffic optimality in mean queue-lengths."
article,2746561,Ankur  Moitra,,,,"Super-resolution, Extremal Functions and the Condition Number of Vandermonde Matrices",821--830,,10,"condition number, extremal functions, super resolution",10.1145/2746539.2746561,,,,,,,2015,,Proceedings of the Forty-seventh Annual ACM Symposium on Theory of Computing,STOC '15,,978-1-4503-3536-2,"Portland, Oregon, USA",ACM,"New York, NY, USA","Super-resolution is a fundamental task in imaging, where the goal is to extract fine-grained structure from coarse-grained measurements. Here we are interested in a popular mathematical abstraction of this problem that has been widely studied in the statistics, signal processing and machine learning communities. We exactly resolve the threshold at which noisy super-resolution is possible. In particular, we establish a sharp phase transition for the relationship between the cutoff frequency (m) and the separation (Δ). If m > 1/Δ + 1, our estimator converges to the true values at an inverse polynomial rate in terms of the magnitude of the noise. And when m < (1-ε) /Δ no estimator can distinguish between a particular pair of Δ-separated signals even if the magnitude of the noise is exponentially small. Our results involve making novel connections between extremal functions and the spectral properties of Vandermonde matrices. We establish a sharp phase transition for their condition number which in turn allows us to give the first noise tolerance bounds for the matrix pencil method. Moreover we show that our methods can be interpreted as giving preconditioners for Vandermonde matrices, and we use this observation to design faster algorithms for super-resolution. We believe that these ideas may have other applications in designing faster algorithms for other basic tasks in signal processing."
article,3084449,Lei  Ying,,,,Stein's Method for Mean Field Approximations in Light and Heavy Traffic Regimes,12:1--12:27,12,27,"heavy-traffic analysis, mean-field models, perturbation theory, stein's method",10.1145/3084449,Proc. ACM Meas. Anal. Comput. Syst.,June 2017,1,1,,June,2017,2476-1249,,,,,,ACM,"New York, NY, USA","Mean-field analysis is an analytical method for understanding large-scale stochastic systems such as large-scale data centers and communication networks. The idea is to approximate the stationary distribution of a large-scale stochastic system using the equilibrium point (called the mean-field limit) of a dynamical system (called the mean-field model). This approximation is often justified by proving the weak convergence of stationary distributions to its mean-field limit. Most existing mean-field models concerned the light-traffic regime where the load of the system, denote by ρ, is strictly less than one and is independent of the size of the system. This is because a traditional mean-field model represents the limit of the corresponding stochastic system. Therefore, the load of the mean-field model is ρ= limN→ ∞ ρ(N), where ρ(N) is the load of the stochastic system of size N. Now if ρ(N)→ 1 as N→ ∞ (i.e., in the heavy-traffic regime), then ρ=1. For most systems, the mean-field limits when ρ=1 are trivial and meaningless. To overcome this difficulty of traditional mean-field models, this paper takes a different point of view on mean-field models. Instead of regarding a mean-field model as the limiting system of large-scale stochastic system, it views the equilibrium point of the mean-field model, called a mean-field solution, simply as an approximation of the stationary distribution of the finite-size system. Therefore both mean-field models and solutions can be functions of N. This paper first outlines an analytical method to bound the approximation error based on Stein's method and the perturbation theory. We further present two examples: the M/M/N queueing system and the supermarket model under the power-of-two-choices algorithm. For both applications, the method enables us to characterize the system performance under a broad range of traffic loads. For the supermarket model, this is the first paper that rigorously quantifies the steady-state performance of the-power-of-two-choices in the heavy-traffic regime. These results in the heavy-traffic regime cannot be obtained using the traditional mean-field analysis and the interchange of the limits."
article,1959383,Patrick  Loiseau and Paulo  Gon&#231;alves and Guillaume  Dewaele and Pierre  Borgnat and Patrice  Abry and Pascale Vicat-Blanc Primet,,,,Investigating Self-similarity and Heavy-tailed Distributions on a Large-scale Experimental Facility,1261--1274,,14,"heavy-tailed distributions, large-scale experiments, monitoring, network traffic, self-similarity",10.1109/TNET.2010.2042726,IEEE/ACM Trans. Netw.,August 2010,18,4,,August,2010,1063-6692,,,,,,IEEE Press,"Piscataway, NJ, USA","After the seminal work by Taqqu et al. relating self-similarity to heavy-tailed distributions, a number of research articles verified that aggregated Internet traffic time series show self-similarity and that Internet attributes, like Web file sizes and flow lengths, were heavy-tailed. However, the validation of the theoretical prediction relating self-similarity and heavy tails remains unsatisfactorily addressed, being investigated using either numerical or network simulations, or from uncontrolled Web traffic data. Notably, this prediction has never been conclusively verified on real networks using controlled and stationary scenarios, prescribing specific heavy-tailed distributions, and estimating confidence intervals. With this goal in mind, we use the potential and facilities offered by the large-scale, deeply reconfigurable and fully controllable experimental Grid5000 instrument, combined with state-of-the-art estimators, to investigate the prediction's observability on real networks. To this end, we organize a large number of controlled traffic circulation sessions on a nationwide real network involving 200 independent hosts. We use a FPGA-based measurement system to collect the corresponding traffic at packet level. We then estimate both the self-similarity exponent of the aggregated time series and the heavy-tail index of flow-size distributions, independently. Not only do our results complement and validate, with a striking accuracy, some conclusions drawn from a series of pioneering studies, but they also bring in new insights on the controversial role of certain components of real networks."
article,2797161,Primo&#382;  Poto&#269;nik and Edvard  Govekar,,,,Practical Considerations in Training Extreme Learning Machines,1:1--1:5,1,5,"Extreme learning machines, activation functions, feedforward neural networks, function approximation, input range scaling",10.1145/2797143.2797161,,,,,,,2015,,Proceedings of the 16th International Conference on Engineering Applications of Neural Networks (INNS),EANN '15,,978-1-4503-3580-5,"Rhodes, Island, Greece",ACM,"New York, NY, USA","Extreme learning machines (ELM) represent a new fast learning algorithm for single layer feedforward networks. In this paper, we investigate several practical properties of training ELM in the context of a simulated function approximation task. ELM with different hidden layer activation functions and varying number of hidden nodes are applied in the learning task and the function approximation accuracy is examined with respect to the range scaling of input variables. The results demonstrate that ELM models are very sensitive with respect to proper input scaling. The approximate range of optimal ELM performance covers only input range scaling within an order of magnitude. Comparison with classical feedforward neural networks with sigmoidal activation functions shows that these are not affected by input range scaling. Results encourage further studies of practical aspect of efficient training and applying ELM."
article,2745878,Rahul  Singh and Alexander  Stolyar,,,,MaxWeight Scheduling: Asymptotic Behavior of Unscaled Queue-Differentials in Heavy Traffic,431--432,,2,"dynamic scheduling, heavy traffic asymptotic regime, markov chain, maxweight algorithm, queue length differentials, smooth service process",10.1145/2796314.2745878,SIGMETRICS Perform. Eval. Rev.,June 2015,43,1,,June,2015,0163-5999,,,,,,ACM,"New York, NY, USA","The model is a ""generalized switch"", serving multiple traffic flows in discrete time. The switch uses MaxWeight algorithm to make a service decision (scheduling choice) at each time step, which determines the probability distribution of the amount of service that will be provided. We are primarily motivated by the following question: in the heavy traffic regime, when the switch load approaches critical level, will the service processes provided to each flow remain ""smooth"" (i.e., without large gaps in service)? Addressing this question reduces to the analysis of the asymptotic behavior of the unscaled queue-differential process in heavy traffic. We prove that the stationary regime of this process converges to that of a positive recurrent Markov chain, whose structure we explicitly describe. This in turn implies asymptotic ""smoothness"" of the service processes."
article,2745878,Rahul  Singh and Alexander  Stolyar,,,,MaxWeight Scheduling: Asymptotic Behavior of Unscaled Queue-Differentials in Heavy Traffic,431--432,,2,"dynamic scheduling, heavy traffic asymptotic regime, markov chain, maxweight algorithm, queue length differentials, smooth service process",10.1145/2745844.2745878,,,,,,,2015,,Proceedings of the 2015 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems,SIGMETRICS '15,,978-1-4503-3486-0,"Portland, Oregon, USA",ACM,"New York, NY, USA","The model is a ""generalized switch"", serving multiple traffic flows in discrete time. The switch uses MaxWeight algorithm to make a service decision (scheduling choice) at each time step, which determines the probability distribution of the amount of service that will be provided. We are primarily motivated by the following question: in the heavy traffic regime, when the switch load approaches critical level, will the service processes provided to each flow remain ""smooth"" (i.e., without large gaps in service)? Addressing this question reduces to the analysis of the asymptotic behavior of the unscaled queue-differential process in heavy traffic. We prove that the stationary regime of this process converges to that of a positive recurrent Markov chain, whose structure we explicitly describe. This in turn implies asymptotic ""smoothness"" of the service processes."
article,948939,Sem  Borst and Michel  Mandjes and Miranda  van Uitert,,,,Generalized Processor Sharing with Light-tailed and Heavy-tailed Input,821--834,,14,"Generalized processor sharing (GPS), Markov fluid, heavy-tailed traffic, large deviations, light-tailed traffic, regular variation, weighted fair queueing, workload asymptotics",10.1109/TNET.2003.818195,IEEE/ACM Trans. Netw.,October 2003,11,5,,October,2003,1063-6692,,,,,,IEEE Press,"Piscataway, NJ, USA","We consider a queue fed by a mixture of light-tailed and heavy-tailed traffic. The two traffic flows are served in accordance with the generalized processor sharing (GPS) discipline. GPS-based scheduling algorithms, such as weighted fair queueing, have emerged as an important mechanism for achieving service differentiation in integrated networks. We derive the asymptotic workload behavior of the light-tailed traffic flow under the assumption that its GPS weight is larger than its traffic intensity. The GPS mechanism ensures that the workload is bounded above by that in an isolated system with the light-tailed flow served in isolation at a constant rate equal to its GPS weight. We show that the workload distribution is in fact asymptotically equivalent to that in the isolated system, multiplied with a certain pre-factor, which accounts for the interaction with the heavy-tailed flow. Specifically, the pre-factor represents the probability that the heavy-tailed flow is backlogged long enough for the light-tailed flow to reach overflow. The results provide crucial qualitative insight in the typical overflow scenario."
article,2834978,Bin  Dong and Surendra  Byna and Kesheng  Wu,,,,Heavy-tailed Distribution of Parallel I/O System Response Time,37--42,,6,"heavy-tailed distribution, parallel I/O, performance and benchmarking, peta-scale storage system, response time",10.1145/2834976.2834978,,,,,,,2015,,Proceedings of the 10th Parallel Data Storage Workshop,PDSW '15,,978-1-4503-4008-3,"Austin, Texas",ACM,"New York, NY, USA","Estimating I/O time of applications is critical for computing system research and developments, such as performance tuning and job scheduling. Parallel I/O systems on large-scale HPC systems typically use several I/O servers attached to a number of hard disk drives to read and write data concurrently. As a result, the response time of individual I/O servers affects the overall I/O performance and modeling the response time distribution holds the key to estimate I/O time. Existing studies have generally considered that the response time follows a Uniform or a Normal distribution. However, none of these studies considered supercomputing environments that are actively used by a number of users to verify the existence of Uniform or Normal distributions. In this study, we collected ≈ 2,500,000 measurements on two peta-scale class supercomputers that are actively used by ≈5000 users. These two systems, Hopper and Edison at the National Energy Research Scientific Computing Center (NERSC), typically support hundreds of concurrent jobs. Our performance measurements include the overheads introduced by the entire parallel I/O stack (I/O library, network, parallel file system software, cache and hardware). Our study shows that the response time of parallel I/O system follows a heavy-tailed property, in contrary to the widely accepted Normal or Uniform distributions. In exploring for new models, we identify that a mix of Power Law and Normal distributions is a good fit for the response time of parallel I/O systems that are actively used by hundreds of jobs concurrently."
article,1028802,Yin  Zhang and Sumeet  Singh and Subhabrata  Sen and Nick  Duffield and Carsten  Lund,,,,"Online Identification of Hierarchical Heavy Hitters: Algorithms, Evaluation, and Applications",101--114,,14,"change detection, data stream computation, hierarchical heavy hitters, network anomaly detection, packet classification",10.1145/1028788.1028802,,,,,,,2004,,Proceedings of the 4th ACM SIGCOMM Conference on Internet Measurement,IMC '04,,1-58113-821-0,"Taormina, Sicily, Italy",ACM,"New York, NY, USA","In traffic monitoring, accounting, and network anomaly detection, it is often important to be able to detect high-volume traffic clusters in near real-time. Such heavy-hitter traffic clusters are often hierarchical (<i>ie</i>, they may occur at different aggregation levels like ranges of IP addresses) and possibly multidimensional (<i>ie</i>, they may involve the combination of different IP header fields like IP addresses, port numbers, and protocol). Without prior knowledge about the precise structures of such traffic clusters, a naive approach would require the monitoring system to examine all possible ombinations of aggregates in order to detect the heavy hitters, which can be proohibitive in terms of computation resources. In this paper, we focus on online identification of 1-dimensional and 2-dimensional hierarchical heavy hitters (HHHs), arguably the two most important scenarios in traffic analysis. We show that the problem of HHH detection can be transformed to one of dynamic packet classification by taking a top-down approach and adaptively creating new rules to match HHHs. We then adapt several existing static packet classification algorithms to support dynamic packet classification. The resulting HHH detection algorithms have much lower worst-case update costs than existing algorithms and can provide tunable deterministic accuracy guarantees. As an application of these algorithms, we also propose robust techniques to detect changes among heavy-hitter traffic clusters. Our techniques can accommodate variability due to sampling that is increasingly used in network measurement. Evaluation based on real Internet traces collected at a Tier-1 ISP suggests that these techniques are remarkably accurate and efficient."
article,2901466,Siva Theja  Maguluri and Sai Kiran  Burle and R.  Srikant,,,,Optimal Heavy-Traffic Queue Length Scaling in an Incompletely Saturated Switch,13--24,,12,"drift method, heavy traffic optimality, nxn switch, performance analysis",10.1145/2964791.2901466,SIGMETRICS Perform. Eval. Rev.,June 2016,44,1,,June,2016,0163-5999,,,,,,ACM,"New York, NY, USA","We consider an input queued switch operating under the MaxWeight scheduling algorithm. This system is interesting to study because it is a model for Internet routers and data center networks. Recently, it was shown that the MaxWeight algorithm has optimal heavy-traffic queue length scaling when all ports are uniformly saturated. Here we consider the case where a fraction of the ports are saturated and others are not (which we call the incompletely saturated case), and also the case where the rates at which the ports are saturated can be different. We use a recently developed drift technique to show that the heavy-traffic queue length under the MaxWeight scheduling algorithm has optimal scaling with respect to the switch size even in these cases."
article,2901466,Siva Theja  Maguluri and Sai Kiran  Burle and R.  Srikant,,,,Optimal Heavy-Traffic Queue Length Scaling in an Incompletely Saturated Switch,13--24,,12,"drift method, heavy traffic optimality, nxn switch, performance analysis",10.1145/2896377.2901466,,,,,,,2016,,Proceedings of the 2016 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Science,SIGMETRICS '16,,978-1-4503-4266-7,"Antibes Juan-les-Pins, France",ACM,"New York, NY, USA","We consider an input queued switch operating under the MaxWeight scheduling algorithm. This system is interesting to study because it is a model for Internet routers and data center networks. Recently, it was shown that the MaxWeight algorithm has optimal heavy-traffic queue length scaling when all ports are uniformly saturated. Here we consider the case where a fraction of the ports are saturated and others are not (which we call the incompletely saturated case), and also the case where the rates at which the ports are saturated can be different. We use a recently developed drift technique to show that the heavy-traffic queue length under the MaxWeight scheduling algorithm has optimal scaling with respect to the switch size even in these cases."
article,1958811,Natalia M. Markovich and Udo R. Krieger,,,,Analyzing Measurements from Data with Underlying Dependences and Heavy-tailed Distributions,425--436,,12,"data analysis, heavy-tailed distributions, long-range dependence, ngn traffic characterization, peer-to-peer packet traffic",10.1145/1958746.1958811,,,,,,,2011,,Proceedings of the 2Nd ACM/SPEC International Conference on Performance Engineering,ICPE '11,,978-1-4503-0519-8,"Karlsruhe, Germany",ACM,"New York, NY, USA","We consider measurements that are arising from a next generation network and present advanced mathematical techniques to cope with the analysis and modeling of the gathered data. These statistical techniques are required to study important performance indices of new real-time services in a multimedia Internet such as the demanded bandwidth or delay-loss profiles of packet flows during a session. The latter data sets incorporate strongly correlated or long-range dependent time series and heavy-tailed marginal distributions determining the underlying random variables of the data features. To illustrate the proposed statistical analysis concept, we use traces arising from the popular peer-to-peer video streaming application SopCast."
article,2851882,Xiao-jian  Ding and Xiao-guang  Liu and Xin  Xu,,,,An Optimization Method of Extreme Learning Machine for Regression,891--893,,3,"extreme learning machine, optimization, regression",10.1145/2851613.2851882,,,,,,,2016,,Proceedings of the 31st Annual ACM Symposium on Applied Computing,SAC '16,,978-1-4503-3739-7,"Pisa, Italy",ACM,"New York, NY, USA","In this paper, we investigate an optimization scheme for extreme learning machine (ELM) regression, named OELR, to overcome the limitation of ELM that it may lead to overfitting on large training data sets. OELR amounts to minimization of ε-insensitive loss and minimization of the norm of the output weights of single hidden layer feedforward networks (SLFNs). Compared to support vector regression (SVR), OELR has less optimization constraints. Empirical results on the benchmark data sets show that the competitive performance of the OELR over the state-ofthe-art regression learning algorithms."
article,1068071,Joshua L. Payne and Margaret J. Eppstein,,,,A Hybrid Genetic Algorithm with Pattern Search for Finding Heavy Atoms in Protein Crystals,377--384,,8,"crystallographic phasing, crystallography, genetic algorithms, heavy atom method, hybrid evolutionary algorithms, isomorphous replacement, pattern search, phase problem",10.1145/1068009.1068071,,,,,,,2005,,Proceedings of the 7th Annual Conference on Genetic and Evolutionary Computation,GECCO '05,,1-59593-010-8,"Washington DC, USA",ACM,"New York, NY, USA","One approach for determining the molecular structure of proteins is a technique called iso-morphous replacement, in which crystallographers dope protein crystals with heavy atoms, such as mercury or platinum. By comparing measured amplitudes of diffracted x-rays through protein crystals with and without the heavy atoms, the locations of the heavy atoms can be estimated. Once the locations of the heavy atoms are known, the phases of the diffracted x-rays through the protein crystal can be estimated, which in turn enables the structure of the protein to be estimated. Unfortunately, the key step in this process is the estimation of the locations of the heavy atoms, and this is a multi-modal, non-linear inverse problem. We report results of a pilot study that show that a 2-stage hybrid algorithm, using a stochastic genetic algorithm for stage 1 followed by a deterministic pattern search algorithm for stage 2, can successfully locate up to 5 heavy atoms in computer simulated crystals using noise free data. We conclude that the method may be a viable approach for finding heavy atoms in protein crystals, and suggest ways in which the approach can be scaled up to larger problems."
article,1989541,Thomas  Locher,,,,Finding Heavy Distinct Hitters in Data Streams,299--308,,10,"anomaly detection, heavy distinct hitter, network monitoring, space complexity, streaming algorithms",10.1145/1989493.1989541,,,,,,,2011,,Proceedings of the Twenty-third Annual ACM Symposium on Parallelism in Algorithms and Architectures,SPAA '11,,978-1-4503-0743-7,"San Jose, California, USA",ACM,"New York, NY, USA","A simple indicator for an anomaly in a network is a rapid increase in the total number of distinct network connections. While it is fairly easy to maintain an accurate estimate of the current total number of distinct connections using streaming algorithms that exhibit both a low space and computational complexity, identifying the network entities that are involved in the largest number of distinct connections efficiently is considerably harder. In this paper, we study the problem of finding all entities whose number of distinct (outgoing or incoming) network connections is at least a specific fraction of the total number of distinct connections. These entities are referred to as heavy distinct hitters. Since this problem is hard in general, we focus on randomized approximation techniques and propose a sampling-based and a sketch-based streaming algorithm. Both algorithms output a list of the potential heavy distinct hitters including the estimated counts of the corresponding number of distinct connections. We prove that, depending on the required level of accuracy of the output list, the space complexities of the presented algorithms are asymptotically optimal up to small logarithmic factors. Additionally, the algorithms are evaluated and compared using real network data in order to determine their usefulness in practice."
article,3036354,Bhakti Yudho Suprapto and M. Ary Heryanto and Herwin  Suprijono and Benyamin  Kusumoputro,,,,Altitude Control of Heavy-Lift Hexacopter Using Direct Inverse Control Based on Elman Recurrent Neural Network,135--140,,6,"Altitude Control, DIC, Elman Recurrent Neural Networks, Heavy-Lift Hexacopter, Neural Networks",10.1145/3036331.3036354,,,,,,,2017,,Proceedings of the 8th International Conference on Computer Modeling and Simulation,ICCMS '17,,978-1-4503-4816-4,"Canberra, Australia",ACM,"New York, NY, USA","This paper proposes the use of Direct Inverse Control (DIC) with Elman Recurrent Neural Network (ERNN) learning algorithm for the altitude control of a heavy-lift hexacopter. The study was conducted analytically using the real flight data obtained from real plant experiment. The results showed that the ERNN can successfully control the altitude of the heavy-lift hexacopter, where the response generated by the DIC system was in good agreement with the test data with low error. Furthermore, the proposed DIC system can also control the attitude, e.g. roll, pitch and yaw of the hexacopter which are also crucial for the hexacopter movement control."
article,2591572,Cory  Merkel and Dhireesha  Kudithipudi,,,,A Current-mode CMOS/Memristor Hybrid Implementation of an Extreme Learning Machine,241--242,,2,"current-mode, extreme learning machine, memristor",10.1145/2591513.2591572,,,,,,,2014,,Proceedings of the 24th Edition of the Great Lakes Symposium on VLSI,GLSVLSI '14,,978-1-4503-2816-6,"Houston, Texas, USA",ACM,"New York, NY, USA","In this work, we propose a current-mode CMOS/memristor hybrid implementation of an extreme learning machine (ELM) architecture. We present novel circuit designs for linear, sigmoid,and threshold neuronal activation functions, as well as memristor-based bipolar synaptic weighting. In addition, this work proposes a stochastic version of the least-mean-squares (LMS) training algorithm for adapting the weights between the ELM's hidden and output layers. We simulated our top-level ELM architecture using Cadence AMS Designer with 45 nm CMOS models and an empirical piecewise linear memristor model based on experimental data from an HfOx device. With 10 hidden node neurons, the ELM was able to learn a 2-input XOR function after 150 training epochs."
article,2631776,Swati  Agarwal and Ashish  Sureka,,,,A Focused Crawler for Mining Hate and Extremism Promoting Videos on YouTube.,294--296,,3,"hate &#38; extremism detection, online radicalization",10.1145/2631775.2631776,,,,,,,2014,,Proceedings of the 25th ACM Conference on Hypertext and Social Media,HT '14,,978-1-4503-2954-5,"Santiago, Chile",ACM,"New York, NY, USA","Online video sharing platforms such as YouTube contains several videos and users promoting hate and extremism. Due to low barrier to publication and anonymity, YouTube is misused as a platform by some users and communities to post negative videos disseminating hatred against a particular religion, country or person. We formulate the problem of identification of such malicious videos as a search problem and present a focused-crawler based approach consisting of various components performing several tasks: search strategy or algorithm, node similarity computation metric, learning from exemplary profiles serving as training data, stopping criterion, node classifier and queue manager. We implement a best-first search algorithm and conduct experiments to measure the accuracy of the proposed approach. Experimental results demonstrate that the proposed approach is effective."
article,2464495,Derek  O'Callaghan and Derek  Greene and Maura  Conway and Joe  Carthy and P&#225;draig  Cunningham,,,,Uncovering the Wider Structure of Extreme Right Communities Spanning Popular Online Networks,276--285,,10,"extreme right, heterogeneous online networks, social network analysis",10.1145/2464464.2464495,,,,,,,2013,,Proceedings of the 5th Annual ACM Web Science Conference,WebSci '13,,978-1-4503-1889-1,"Paris, France",ACM,"New York, NY, USA","Recent years have seen increased interest in the online presence of extreme right groups. Although originally composed of dedicated websites, the online extreme right milieu now spans multiple networks, including popular social media platforms such as Twitter, Facebook and YouTube. Ideally therefore, any contemporary analysis of online extreme right activity requires the consideration of multiple data sources, rather than being restricted to a single platform. We investigate the potential for Twitter to act as one possible gateway to communities within the wider online network of the extreme right, given its facility for the dissemination of content. A strategy for representing heterogeneous network data with a single homogeneous network for the purpose of community detection is presented, where these inherently dynamic communities are tracked over time. We use this strategy to discover and analyze persistent English and German language extreme right communities."
article,3152743,Chen Chai Phing and Tiong Sieh Kiong and Md Fauzan K. Mohd Yapandi and Johnny Koh Siaw Paw,,,,Prediction of NO2 Emission Concentration via Correlation of Multiple Big Data Sources Using Extreme Learning Machine,23--27,,5,"Emission Prediction, Extreme Learning Machine, Meteorology, Terrain Profile",10.1145/3152723.3152743,,,,,,,2017,,Proceedings of the 2017 International Conference on Big Data Research,ICBDR 2017,,978-1-4503-5356-4,"Osaka, Japan",ACM,"New York, NY, USA","Increase of electricity demand and urbanization process has caused more power plants to be built to meet the demand of electricity. However, development of power plant will cause environmental issue for its surrounding. Necessary measures need to be taken to ensure social and environmental sustainability. Among the requirements in Malaysia, discharge of air pollution emission of a gas- or distillate-fired power plant has to comply with air pollution level as described in the Malaysian Ambient Air Quality Standards ((MAAQS) 2013 and the Environmental Quality (Clean Air) Regulations 2014. Pertaining to the environmental requirements, this paper is to investigate the ability of a regression based artificial intelligence tool, namely Extreme Learning Machine (ELM) in correlating multiple sources of big data sets and subsequently predicting the air pollution emission level from the chimney of a Combined Cycle Gas Turbine (CCGT) power plant. This emission data is later being used to ensure the clean air regulatory requirement is fulfilled. The big data sources that have been used in this work are meteorological data, terrain and land use data, historical emission data and power plant parameters particularly related to the point source emitter. With the correlation of multiple big data sources, Extreme Learning Machine (ELM) is then trained for the prediction of emission rate at certain targeted areas, which are classified as air sensitive receptors (ASR) surrounding the power plant. Nitrogen dioxide (NO2) is the key emission that has been studied in this paper due to its criticality towards environment. A standalone application program has been developed to employ ELM based big data analytics tool for the prediction of NO2 pollution emission. The output of ELM is analyzed to ensure the emission at ground level of ASR is maintained within allowable limit."
article,2077576,Ming-Chun  Huang and Ethan  Chen and Wenyao  Xu and Majid  Sarrafzadeh,,,,Gaming for Upper Extremities Rehabilitation,27:1--27:2,27,2,"3D camera, IMU, Kinect, calibration, data glove, exercise, fusion, gaming, health, upper extremities rehabilitation",10.1145/2077546.2077576,,,,,,,2011,,Proceedings of the 2Nd Conference on Wireless Health,WH '11,,978-1-4503-0982-0,"San Diego, California",ACM,"New York, NY, USA","This paper presents a hand motion capture system. Its application is a wearable controller for an upper extremities rehabilitation game. The system consists of a wearable data glove platform (SmartGlove, a customized finger pressure, bending, and 9DOM motion extraction platform) and a 3D camera vision device (Kinect [1], vision based game controller supplied by Microsoft) to extract detailed upper extremities parameters as gaming inputs. The testing application is a jewel thief game [2] implemented with C# and Unity, which requires the tester, such as a stroked patient with upper extremities disabilities to perform a series of predefined upper extremities movements to accomplish the assigned jewel grasping tasks. The parameters and timing could recorded during the gaming process for further medical analysis. This system is targeted on lowering the cost of rehabilitating impaired limbs, providing remote patient monitoring, and acting as a natural interface for gaming systems. In addition, the extracted parameters can be exploited to reconstruct of an accurate model of the patient's limbs and body. Therefore, the proposed system can provide remote medical staffs a wide variety of information to aid a patient's rehabilitation, including, but not limited to aiding in progress evaluations and direct rehabilitative exercises and entertainment."
article,2638741,Anthony  Faiola and Preethi  Srinivas,,,,Extreme Mediation: Observing Mental and Physical Health in Everyday Life,47--50,,4,"conscious awareness, extreme mediation, mental health, physical activity, smartphone overuse",10.1145/2638728.2638741,,,,,,,2014,,Proceedings of the 2014 ACM International Joint Conference on Pervasive and Ubiquitous Computing: Adjunct Publication,UbiComp '14 Adjunct,,978-1-4503-3047-3,"Seattle, Washington",ACM,"New York, NY, USA","The excessive use of smartphones resulting in extreme mediation has been identified to result in psychological problems including anxiety, depression, and an overall neural change that is impacting people of all ages on many levels. An exploratory study using Experience Sampling Method (ESM) concluded a significant increase in positive mood, conscious awareness of the surrounding environment, and an increased number of participants self-reporting physical activity lasting 15 minutes on days without smartphone use. Results suggest the need to avoid increased use of noninvasive technology such as smartphones resulting in deterioration of mental and physical health."
article,3084218,Ivanoe  De Falco and Eryk  Laskowski and Richard  Olejnik and Umberto  Scafuri and Ernesto  Tarantino and Marek  Tudruj,,,,Multi-objective Parallel Extremal Optimization in Processor Load Balancing for Distributed Programs,1796--1803,,8,"extremal optimization, multi-objective optimization, processor load balancing",10.1145/3067695.3084218,,,,,,,2017,,Proceedings of the Genetic and Evolutionary Computation Conference Companion,GECCO '17,,978-1-4503-4939-0,"Berlin, Germany",ACM,"New York, NY, USA","The paper concerns multi-objective methodology applied to parallel Extremal Optimization (EO) used in processor load balancing in execution of distributed programs. When load imbalance is detected in executive processors then EO algorithms are used to find best tasks migration leading to imbalance reduction and improvement of program execution time. For this a special multi-objective version of parallel EO is applied. It is based on the EO Guided Search (EO-GS) approach which employs problem knowledge to search for the best next solution state in solution improvement. In this EO version, additional fitness function is used in stochastic selection of next solution state based on computation and communication assessment of task migration targets. In the multi-objective EO approach we jointly control three objectives relevant in processor load balancing for distributed applications. They are: computational load balance in execution of distributed applications, volume of communication between tasks on different processors and task migration parameters which fight imbalance of processor loads. The proposed algorithms are assessed by simulated execution of distributed programs macro data flow graphs."
article,2481430,Yin  Lu and Yong  Chen and Yu  Zhuang and Rajeev  Thakur,,,,Memory-conscious Collective I/O for Extreme Scale HPC Systems,5:1--5:8,5,8,"collective I/O, extreme scale system, high performance computing, many-core architecture, parallel I/O",10.1145/2491661.2481430,,,,,,,2013,,Proceedings of the 3rd International Workshop on Runtime and Operating Systems for Supercomputers,ROSS '13,,978-1-4503-2146-4,"Eugene, Oregon",ACM,"New York, NY, USA","Upcoming extreme scale platforms are expected to have millions of nodes with hundreds to thousands of small cores for each node. The continuing decrease in memory capacity per core and the increasing disparity between core count and off-chip memory bandwidth can lead to significant challenges for I/O operations in extreme scale systems. Collective I/O is a critical I/O optimization technique, and the extreme scale challenges require rethinking this strategy for the effective exploitation of the correlation among I/O accesses. In this study, considering the constraint of the memory capacity and bandwidth, we introduce a Memory-Conscious Collective I/O. The new collective I/O strategy restricts aggregation data traffic within disjointed subgroups, coordinates I/O accesses in intra-node and inter-node layer, and determines I/O aggregators at run time considering memory consumption and variance among processes. The preliminary results have demonstrated that this strategy holds promise in mitigating the memory pressure, alleviating the contention for memory bandwidth, and improving the I/O performance for projected extreme scale HPC systems."
article,2413122,Ming-Chun  Huang and Wenyao  Xu and Yi  Su and Belinda  Lange and Chien-Yen  Chang and Majid  Sarrafzadeh,,,,SmartGlove for Upper Extremities Rehabilitative Gaming Assessment,20:1--20:4,20,4,"Kinect, animation, data glove, rehabilitation, upper extremities, upper limbs, video game",10.1145/2413097.2413122,,,,,,,2012,,Proceedings of the 5th International Conference on PErvasive Technologies Related to Assistive Environments,PETRA '12,,978-1-4503-1300-1,"Heraklion, Crete, Greece",ACM,"New York, NY, USA","This paper presents a quantitative assessment solution for an upper extremities rehabilitative gaming application [1]. This assessment solution consists of a set of stand-alone hardware, including SmartGlove and Kinect, a depth capturing sensor made by Microsoft. SmartGlove is a specially designed motion and finger angle extraction device which is packaged in an easy-to-wear and adjustable manner for a patient with an upper extremity impairment. Sensor data extraction, alignment, and visualization algorithms were designed for integrating hand-mounted sensors data streams into skeleton coordinates captured by the Kinect. This enhanced skeleton information can be summarized and replayed as upper extremity joint coordinate animations which can be used for physical therapists to quantify rehabilitation progress. In addition, to serve as an assessment tool, enhanced skeleton information can be used to extend the capability of the Kinect vision system, such as providing motion capture of the upper extremities, even when the testing subject is out of camera scope or one's upper extremities are occluded by the body."
article,3078592,Lei  Ying,,,,Stein's Method for Mean-Field Approximations in Light and Heavy Traffic Regimes,49--49,,1,"Stein's method, heavy traffic analysis, large-scale stochastic systems, mean-field approximations, the-power-of-two-choices",10.1145/3143314.3078592,SIGMETRICS Perform. Eval. Rev.,June 2017,45,1,,June,2017,0163-5999,,,,,,ACM,"New York, NY, USA","Mean-field analysis is an analytical method for understanding large-scale stochastic systems such as large-scale data centers and communication networks. The idea is to approximate the stationary distribution of a large-scale stochastic system using the equilibrium point (called the mean-field limit) of a dynamical system (called the mean-field model). This approximation is often justified by proving the weak convergence of stationary distributions to its mean-field limit. Most existing mean-field models concerned the light-traffic regime where the load of the system, denote by ρ, is strictly less than one and is independent of the size of the system. This is because a traditional mean-field model represents the limit of the corresponding stochastic system. Therefore, the load of the mean-field model is ρ=limN-> ∞ ρ(N), where ρ(N) is the load of the stochastic system of size N. Now if ρ(N)-> 1 as N -> ∞ (i.e., in the heavy-traffic regime), then ρ=1. For most systems, the mean-field limits when ρ=1 are trivial and meaningless. To overcome this difficulty of traditional mean-field models, this paper takes a different point of view on mean-field models. Instead of regarding a mean-field model as the limiting system of large-scale stochastic system, it views the equilibrium point of the mean-field model, called a mean-field solution, simply as an approximation of the stationary distribution of the finite-size system. Therefore both mean-field models and solutions can be functions of N. The proposed method focuses on quantifying the approximation error. If the approximation error is small (as we will show in two applications), then we can conclude that the mean-field solution is a good approximation of the stationary distribution."
article,3078592,Lei  Ying,,,,Stein's Method for Mean-Field Approximations in Light and Heavy Traffic Regimes,49--49,,1,"Stein's method, heavy traffic analysis, large-scale stochastic systems, mean-field approximations, the-power-of-two-choices",10.1145/3078505.3078592,,,,,,,2017,,Proceedings of the 2017 ACM SIGMETRICS / International Conference on Measurement and Modeling of Computer Systems,SIGMETRICS '17 Abstracts,,978-1-4503-5032-7,"Urbana-Champaign, Illinois, USA",ACM,"New York, NY, USA","Mean-field analysis is an analytical method for understanding large-scale stochastic systems such as large-scale data centers and communication networks. The idea is to approximate the stationary distribution of a large-scale stochastic system using the equilibrium point (called the mean-field limit) of a dynamical system (called the mean-field model). This approximation is often justified by proving the weak convergence of stationary distributions to its mean-field limit. Most existing mean-field models concerned the light-traffic regime where the load of the system, denote by ρ, is strictly less than one and is independent of the size of the system. This is because a traditional mean-field model represents the limit of the corresponding stochastic system. Therefore, the load of the mean-field model is ρ=limN-> ∞ ρ(N), where ρ(N) is the load of the stochastic system of size N. Now if ρ(N)-> 1 as N -> ∞ (i.e., in the heavy-traffic regime), then ρ=1. For most systems, the mean-field limits when ρ=1 are trivial and meaningless. To overcome this difficulty of traditional mean-field models, this paper takes a different point of view on mean-field models. Instead of regarding a mean-field model as the limiting system of large-scale stochastic system, it views the equilibrium point of the mean-field model, called a mean-field solution, simply as an approximation of the stationary distribution of the finite-size system. Therefore both mean-field models and solutions can be functions of N. The proposed method focuses on quantifying the approximation error. If the approximation error is small (as we will show in two applications), then we can conclude that the mean-field solution is a good approximation of the stationary distribution."
article,2532591,Xi  Chen and Markus  Koskela,,,,Online RGB-D Gesture Recognition with Extreme Learning Machines,467--474,,8,"RGB-D, extreme learning machine, hog, online gesture recognition, skeleton model",10.1145/2522848.2532591,,,,,,,2013,,Proceedings of the 15th ACM on International Conference on Multimodal Interaction,ICMI '13,,978-1-4503-2129-7,"Sydney, Australia",ACM,"New York, NY, USA","Gesture recognition is needed in many applications such as human-computer interaction and sign language recognition. The challenges of building an actual recognition system do not lie only in reaching an acceptable recognition accuracy but also with requirements for fast online processing. In this paper, we propose a method for online gesture recognition using RGB-D data from a Kinect sensor. Frame-level features are extracted from RGB frames and the skeletal model obtained from the depth data, and then classified by multiple extreme learning machines. The outputs from the classifiers are aggregated to provide the final classification results for the gestures. We test our method on the ChaLearn multi-modal gesture challenge data. The results of the experiments demonstrate that the method can perform effective multi-class gesture recognition in real-time."
article,2998365,Mark&#233;ta  Dolej&#353;ov&#225; and Denisa  Kera,,,,Soylent Diet Self-Experimentation: Design Challenges in Extreme Citizen Science Projects,2112--2123,,12,"extreme citizen science, health, nutrition literacy, risk, self-experimentation, self-tracking",10.1145/2998181.2998365,,,,,,,2017,,Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing,CSCW '17,,978-1-4503-4335-0,"Portland, Oregon, USA",ACM,"New York, NY, USA","Quantified self-experimentation with personal diets is a popular activity among health enthusiasts, diagnosed patients, as well as ""life hackers"" pursuing self-optimization goals. In this paper, we reflect on self-experimentation practices in the context of amateur citizen science communities. We report findings from 11 month-long qualitative fieldwork in a community of nutrition hobbyists experimenting with a powdered food substitute ""soylent"". Our respondents customized the soylent powders to their personal needs, tracked their metabolic reactions to the diet, and discussed their findings with the online soylent user community. Although the data and knowledge sharing within the community positively impacted respondents' nutrition literacy, these activities created risks regarding their health safety and data privacy. We define soylent self-experimentation as a form of ""extreme citizen science"". Based on the limitations identified in the soylent community, we suggest a set of design recommendations for extreme citizen science projects."
article,2393260,G.  Geetha,,,,Detecting Epileptic Seizures Using Electroencephalogram: A Novel Technique for Seizure Classification Using Fast Walsh-Hadamard Transform and Hybrid Extreme Learning Machine,253--260,,8,"ElectroEncephaloGram, Extreme Learning Machine, Hybrid ELM, Levenberg--Marquardt algorithm, analytical hierarchy process, epilepsy",10.1145/2393216.2393260,,,,,,,2012,,"Proceedings of the Second International Conference on Computational Science, Engineering and Information Technology",CCSEIT '12,,978-1-4503-1310-0,"Coimbatore UNK, India",ACM,"New York, NY, USA","Epilepsy is one of the frequent brain disorder that may consequence in the brain dysfunction and cognitive disorders. Epileptic seizures occurred because of the transient and unexpected electrical interruptions of brain. EEG (ElectroEncephaloGram) is one of the most important methods for inquiring the human brain dynamics that affords a direct evaluation of cortical behavior. Data recording create very lengthy data, and therefore the inspection and identification of epilepsy will take more time for completion. In recent years, computerized diagnoses of systems are usually established to make the diagnosis simpler. This paper discusses an implementation of automated epileptic EEG detection system using Fast Walsh-Hadamard Transform and Hybrid Extreme Learning Machine. In this paper, Spatially-Constrained Independent Component Analysis (SCICA) with otsu's thresholding is used to separate the exactly the artificate Independent Components (ICs) from the initial EEG signal. Fast Walsh-Hadamard Transform and Sample Entropy (SampEn) is used for feature extraction to the task of classifying EEG signals, that which are normal, ictal and interictal. This paper uses Hybrid ELM as a classification model. This classification model uses the Analytical Hierarchy Process (AHP) method to select the input weights and hidden biases, the ELM algorithm to analytically determine the output weights and the Levenberg--Marquardt (LM) algorithm to learn the network. Experimental results show that automatic epilepsy detection using Fast Walsh-Hadamard Transform and Hybrid Extreme Learning Machine achieves batter accuracy in lesser time than standard ELM."
